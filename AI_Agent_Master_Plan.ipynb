{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8189c0d8",
   "metadata": {},
   "source": [
    "# AI Agent Master Plan: Virtual Assistant & Task Scheduler\n",
    "\n",
    "**Goal**: Build a deployable, modular AI agent that converges a conversational assistant with a reliable task scheduler.\n",
    "\n",
    "**Initial Automations**:\n",
    "1. **Intelligent Inbox Manager** – sorts, flags, and auto-replies to emails\n",
    "2. **Quick Data-Analysis Engine** – processes datasets and generates LLM summaries\n",
    "\n",
    "**Deliverables**: Cloud/on-prem agent, dashboard + CLI, setup docs, and hand-off package.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **Project Architecture & System Design**\n",
    "2. **Technology Stack Selection & Rationale**\n",
    "3. **Data Flow & Integration Patterns**\n",
    "4. **Modularity & Extension Points**\n",
    "5. **Development Phases & Roadmap**\n",
    "6. **Cost & Infrastructure Planning**\n",
    "7. **Security & Compliance Requirements**\n",
    "8. **Acceptance Testing & Validation**\n",
    "9. **Master TODO List & Scaffolding**\n",
    "10. **Checklists (PR, Release, Hand-off)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b024b",
   "metadata": {},
   "source": [
    "# Section 1: Project Architecture & System Design\n",
    "\n",
    "## High-Level System Diagram\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                        USER INTERFACE                           │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│   Web Dashboard (React/Next.js + TailwindCSS)                   │\n",
    "│   + CLI Admin Tool (Python Typer)                               │\n",
    "└──────────────────────────┬──────────────────────────────────────┘\n",
    "                           │\n",
    "┌──────────────────────────▼──────────────────────────────────────┐\n",
    "│                      API LAYER                                  │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│   FastAPI Backend                                               │\n",
    "│   ├── Auth/OAuth endpoints                                      │\n",
    "│   ├── Conversation endpoint                                     │\n",
    "│   ├── Rules & approval UI endpoints                             │\n",
    "│   ├── Job launcher endpoint                                     │\n",
    "│   ├── Status & logs endpoints                                   │\n",
    "│   └── Webhook handlers                                          │\n",
    "└──────────────────────────┬──────────────────────────────────────┘\n",
    "                           │\n",
    "┌──────────────────────────▼──────────────────────────────────────┐\n",
    "│                 CORE AI ORCHESTRATION                           │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│   LangChain Orchestration                                       │\n",
    "│   ├── Prompt management & chaining                              │\n",
    "│   ├── Memory & conversation history                             │\n",
    "│   ├── Retrieval-augmented generation (RAG)                      │\n",
    "│   └── LLM provider abstraction                                  │\n",
    "│                                                                 │\n",
    "│   Vector Store (FAISS for MVP)                                  │\n",
    "│   ├── Email embeddings + metadata                               │\n",
    "│   ├── Context retrieval for rules                               │\n",
    "│   └── Similarity-based routing                                  │\n",
    "└────┬───────────────────────────────────────────────────────────┬┘\n",
    "     │                                                            │\n",
    "┌────▼──────────────────────────┐  ┌──────────────────────────────▼─┐\n",
    "│  CONNECTORS & DATA SOURCES     │  │  TASK SCHEDULER & WORKER POOL │\n",
    "├────────────────────────────────┤  ├──────────────────────────────┤\n",
    "│ Email Connectors:              │  │  Celery + Redis              │\n",
    "│  ├── IMAP/SMTP                 │  │  ├── Task queues             │\n",
    "│  ├── Gmail API (OAuth)         │  │  ├── Worker pool             │\n",
    "│  └── Outlook API (OAuth)       │  │  ├── Retry policies          │\n",
    "│                                │  │  └── Status tracking         │\n",
    "│ Data Connectors:               │  │                              │\n",
    "│  ├── CSV/Excel upload          │  │  Background Jobs:            │\n",
    "│  ├── Google Sheets             │  │  ├── Email polling           │\n",
    "│  ├── S3                        │  │  ├── Email classification    │\n",
    "│  ├── SQL (Postgres/MySQL)      │  │  ├── Auto-reply sending      │\n",
    "│  └── BigQuery (optional)       │  │  ├── Data analysis           │\n",
    "└────────────────────────────────┘  └──────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Component Responsibilities\n",
    "\n",
    "| Component | Responsibility |\n",
    "|-----------|---|\n",
    "| **Web Dashboard** | UI for inbox preview, template manager, job launcher, logs |\n",
    "| **API Layer** | RESTful endpoints for conversation, job control, approvals |\n",
    "| **LangChain Orchestration** | Chain & prompt management, RAG, LLM integration |\n",
    "| **Vector Store (FAISS)** | Embeddings index for context retrieval, similarity search |\n",
    "| **Email Connectors** | Poll/receive emails, handle OAuth, manage tokens |\n",
    "| **Data Connectors** | Ingest CSV, Google Sheets, S3, SQL sources |\n",
    "| **Task Scheduler (Celery)** | Distribute jobs to workers, manage retries & status |\n",
    "| **Worker Pool** | Execute email tasks, data analysis, template substitution |\n",
    "| **Postgres DB** | Users, rules, approvals, audit logs, config |\n",
    "| **S3-Compatible Storage** | Uploaded datasets, generated reports, logs archive |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642150d",
   "metadata": {},
   "source": [
    "# Section 2: Technology Stack Selection & Rationale\n",
    "\n",
    "## Recommended Stack\n",
    "\n",
    "### Backend & Orchestration\n",
    "\n",
    "| Component | Choice | Rationale | Alternatives |\n",
    "|-----------|--------|-----------|---|\n",
    "| **Language/Framework** | Python + FastAPI | Fast dev, strong async support, strong ecosystem | Flask, Django, Go |\n",
    "| **LLM Orchestration** | LangChain | Excellent connectors, prompt management, chains | LlamaIndex, Semantic Kernel |\n",
    "| **LLM Provider** | OpenAI (start) | Best dev DX, fastest to market | Anthropic, Local LLaMA/Alpaca |\n",
    "\n",
    "**Cost Estimate**: OpenAI at $0.01–$0.10 per email classification; cache to save 50–90% on repeated prompts.\n",
    "\n",
    "**Migration Path**: Start OpenAI → switch to local model (LLaMA 2) or Anthropic for cost/safety after MVP.\n",
    "\n",
    "### Embeddings & Retrieval\n",
    "\n",
    "| Component | Choice | Rationale | Alternatives |\n",
    "|-----------|--------|-----------|---|\n",
    "| **Vector DB** | FAISS (MVP) | Free, in-process, no external dependency | Pinecone, Weaviate, Milvus |\n",
    "| **Scaling** | Pinecone | Managed, auto-scaling, serverless | Weaviate Cloud, Milvus |\n",
    "\n",
    "**Cost**: FAISS = $0 (local), Pinecone = $1–$100/month (manage scale).\n",
    "\n",
    "### Task Scheduling & Workers\n",
    "\n",
    "| Component | Choice | Rationale | Alternatives |\n",
    "|-----------|--------|-----------|---|\n",
    "| **Scheduler** | Celery + Redis | Mature, cheap to host, battle-tested | Prefect, Temporal, APScheduler |\n",
    "| **Message Broker** | Redis | Simple, fast, low ops overhead | RabbitMQ, Amazon SQS |\n",
    "\n",
    "**Cost**: Redis (managed) = $15–$30/month; Celery = free.\n",
    "\n",
    "### Dashboard & CLI\n",
    "\n",
    "| Component | Choice | Rationale | Alternatives |\n",
    "|-----------|--------|-----------|---|\n",
    "| **Dashboard** | Next.js + TailwindCSS | Fast dev, great DX, SEO-friendly | React SPA, Vue, Svelte |\n",
    "| **CLI** | Python Typer | Simple, integrates with backend | Click, Argparse, Clack |\n",
    "\n",
    "**Cost**: $0 (open-source). Hosting = $10–$30/month for static + API.\n",
    "\n",
    "### Storage & Database\n",
    "\n",
    "| Component | Choice | Rationale | Alternatives |\n",
    "|-----------|--------|-----------|---|\n",
    "| **Relational DB** | PostgreSQL | ACID, JSON support, scalable | MySQL, MariaDB, SQLite |\n",
    "| **Object Storage** | S3-compatible (DO Spaces) | Cheap, durable, easy S3 migration | AWS S3, GCS, Backblaze B2 |\n",
    "\n",
    "**Cost**: Postgres (managed) = $15/month; DO Spaces = $5/month (250 GB).\n",
    "\n",
    "### Deployment & Infrastructure\n",
    "\n",
    "| Component | Choice | Rationale | Alternatives |\n",
    "|-----------|--------|-----------|---|\n",
    "| **Containerization** | Docker | Standard, portable, efficient | Podman, containerd |\n",
    "| **MVP Orchestration** | Docker Compose | Simple, single-host, zero ops | Kubernetes, ECS, Nomad |\n",
    "| **Hosting (MVP)** | DigitalOcean Droplet | $6–$12/month, full control, low ops | Render, Railway, AWS, Linode |\n",
    "| **Scaling** | Kubernetes | Managed k8s on DO/AWS/GCP | Docker Compose scale (manual), ECS |\n",
    "\n",
    "**Cost**: Droplet ($12) + Postgres ($15) + Redis ($15) + DO Spaces ($5) = ~$47/month MVP.\n",
    "\n",
    "### Observability & Security\n",
    "\n",
    "| Component | Choice | Rationale | Alternatives |\n",
    "|-----------|--------|-----------|---|\n",
    "| **Error Tracking** | Sentry (free tier) | Easy setup, good context, free tier sufficient | DataDog, Rollbar, Honeycomb |\n",
    "| **Logging** | Postgres + logs → S3 | Queryable, cheap storage | ELK, Grafana Loki, CloudWatch |\n",
    "| **Monitoring** | Prometheus + Grafana | Open-source, self-hosted, flexible | DataDog, New Relic, Honeycomb |\n",
    "| **Secrets Mgmt** | AWS Secrets Manager or Vault | Encrypted, rotatable, audit trail | HashiCorp Vault, 1Password, Doppler |\n",
    "\n",
    "**Cost**: Sentry free + self-hosted Prometheus/Grafana = $0 (self-hosted).\n",
    "\n",
    "### CI/CD\n",
    "\n",
    "| Component | Choice | Rationale | Alternatives |\n",
    "|-----------|--------|-----------|---|\n",
    "| **CI/CD** | GitHub Actions | Free for public repos, integrated, fast | GitLab CI, CircleCI, Jenkins |\n",
    "\n",
    "**Cost**: $0 (free tier on GitHub).\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Breakdown (Annual MVP)\n",
    "\n",
    "| Item | Monthly | Annual |\n",
    "|------|---------|--------|\n",
    "| Compute (DO Droplet) | $12 | $144 |\n",
    "| Database (Postgres managed) | $15 | $180 |\n",
    "| Cache (Redis managed) | $15 | $180 |\n",
    "| Object Storage (DO Spaces) | $5 | $60 |\n",
    "| LLM calls (OpenAI, 10k/month @ avg $0.01) | $100 | $1,200 |\n",
    "| Monitoring (Sentry free tier) | $0 | $0 |\n",
    "| Domain + SSL (Let's Encrypt) | $0 | $0 |\n",
    "| **TOTAL** | **~$147** | **~$1,764** |\n",
    "\n",
    "*Can be reduced to ~$900/year by self-hosting Redis/Postgres on same droplet and using local FAISS.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c68f55",
   "metadata": {},
   "source": [
    "# Section 3: Data Flow & Integration Patterns\n",
    "\n",
    "## Email Linking & OAuth Flow\n",
    "\n",
    "```\n",
    "User → Dashboard \"Link Email\" \n",
    "  → OpenID/OAuth consent screen\n",
    "  → Backend exchanges code for token\n",
    "  → Encrypt token with AES-KMS\n",
    "  → Store in DB with email_id, provider, scope\n",
    "  → Poll mailbox every 5 min (or push event)\n",
    "  → Create \"email_job\" in Celery queue\n",
    "```\n",
    "\n",
    "### OAuth Token Encryption Example\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/app/security/encryption.py\n",
    "\n",
    "from cryptography.fernet import Fernet\n",
    "import os\n",
    "\n",
    "class TokenEncryptor:\n",
    "    def __init__(self, key: str = None):\n",
    "        self.cipher = Fernet(key or os.getenv(\"ENCRYPTION_KEY\"))\n",
    "    \n",
    "    def encrypt_token(self, token: str) -> str:\n",
    "        return self.cipher.encrypt(token.encode()).decode()\n",
    "    \n",
    "    def decrypt_token(self, encrypted: str) -> str:\n",
    "        return self.cipher.decrypt(encrypted.encode()).decode()\n",
    "\n",
    "# In DB schema:\n",
    "# CREATE TABLE user_email_accounts (\n",
    "#     id SERIAL PRIMARY KEY,\n",
    "#     user_id INT REFERENCES users(id),\n",
    "#     provider VARCHAR(20),  -- 'gmail', 'outlook', 'imap'\n",
    "#     email_address VARCHAR(255),\n",
    "#     encrypted_token TEXT,  -- Store encrypted OAuth token\n",
    "#     scopes TEXT,\n",
    "#     created_at TIMESTAMP DEFAULT NOW()\n",
    "# );\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Email Processing Pipeline\n",
    "\n",
    "```\n",
    "Email Job Entry\n",
    "  ↓\n",
    "[Fetch Email] (worker/tasks/email_processor.py)\n",
    "  ├── Connect via provider adapter (Gmail API, Outlook API, IMAP)\n",
    "  ├── Retrieve email metadata + body + attachments\n",
    "  ↓\n",
    "[Embed & Classify] (worker/tasks/classifier.py)\n",
    "  ├── Tokenize subject + body\n",
    "  ├── Generate embeddings (OpenAI embed-3-small)\n",
    "  ├── Query FAISS for similar emails + rules\n",
    "  ├── LangChain chain for classification\n",
    "  ↓\n",
    "[Route Decision] (worker/tasks/router.py)\n",
    "  ├── Rule matches? → Apply label + flag\n",
    "  ├── Confidence > threshold & approved template? → Queue auto-reply\n",
    "  ├── Else → Flag to user, create follow-up task\n",
    "  ↓\n",
    "[Action Execution] (worker/tasks/action_executor.py)\n",
    "  ├── Send auto-reply via SMTP/Gmail API\n",
    "  ├── Mark as processed\n",
    "  ├── Log to audit table\n",
    "```\n",
    "\n",
    "### Email Processor Example\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/worker/tasks/email_processor.py\n",
    "\n",
    "from celery import shared_task\n",
    "from backend.connectors.email import EmailConnectorFactory\n",
    "from backend.models import EmailJob, AuditLog\n",
    "\n",
    "@shared_task(bind=True, max_retries=3)\n",
    "def process_email(self, email_job_id: int):\n",
    "    job = EmailJob.query.get(email_job_id)\n",
    "    user = job.user\n",
    "    \n",
    "    try:\n",
    "        # Get encrypted token, decrypt it\n",
    "        account = user.email_accounts[0]\n",
    "        token = decrypt_token(account.encrypted_token)\n",
    "        \n",
    "        # Get provider adapter\n",
    "        connector = EmailConnectorFactory.create(\n",
    "            provider=account.provider,\n",
    "            token=token\n",
    "        )\n",
    "        \n",
    "        # Fetch email\n",
    "        raw_email = connector.fetch(job.message_id)\n",
    "        \n",
    "        # Classify\n",
    "        embedding = embed_text(raw_email['subject'] + ' ' + raw_email['body'])\n",
    "        rules = find_matching_rules(embedding, user_id=user.id)\n",
    "        \n",
    "        # Route\n",
    "        if rules:\n",
    "            for rule in rules:\n",
    "                apply_label(job, rule['label'])\n",
    "                if rule['action'] == 'auto_reply' and user.approvals[rule['id']]:\n",
    "                    queue_auto_reply_task(job.id, rule['template_id'])\n",
    "        \n",
    "        job.status = 'processed'\n",
    "        job.save()\n",
    "        \n",
    "        AuditLog.create(\n",
    "            user_id=user.id,\n",
    "            action='email_processed',\n",
    "            resource_id=job.id,\n",
    "            details={'rules_applied': len(rules)}\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        self.retry(exc=exc, countdown=60)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Auto-Reply Workflow with Approval Gate\n",
    "\n",
    "```\n",
    "User uploads template\n",
    "  ↓\n",
    "Dashboard: Template editor (subject, body, rules)\n",
    "  ↓\n",
    "Template stored in DB (versioned, with approval flag)\n",
    "  ↓\n",
    "User toggles approval for auto-send\n",
    "  ↓\n",
    "When rule + email match (confidence > threshold):\n",
    "  ├── Check approval flag\n",
    "  ├── Check daily send limit\n",
    "  ├── Check confidence score vs. threshold\n",
    "  ├── If all pass → generate personalized reply (LLM substitution)\n",
    "  ├── Else → flag to user for manual review\n",
    "  ↓\n",
    "Send reply via SMTP/Gmail API\n",
    "  ↓\n",
    "Log action (who approved, when, confidence, email_id, template_id)\n",
    "```\n",
    "\n",
    "### Pre-Approved Reply Example\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/worker/tasks/auto_reply.py\n",
    "\n",
    "from backend.models import Template, AuditLog\n",
    "from backend.security.encryption import encrypt_audit_data\n",
    "\n",
    "@shared_task\n",
    "def send_auto_reply(email_job_id: int, template_id: int):\n",
    "    job = EmailJob.query.get(email_job_id)\n",
    "    template = Template.query.get(template_id)\n",
    "    user = job.user\n",
    "    \n",
    "    # Safety checks\n",
    "    assert template.approved_for_auto_send, \"Template not approved\"\n",
    "    \n",
    "    daily_sent = AuditLog.query.filter(\n",
    "        AuditLog.user_id == user.id,\n",
    "        AuditLog.action == 'auto_reply_sent',\n",
    "        AuditLog.created_at >= datetime.now() - timedelta(days=1)\n",
    "    ).count()\n",
    "    \n",
    "    assert daily_sent < user.settings.max_daily_replies, \"Daily limit reached\"\n",
    "    assert job.classifier_confidence > 0.85, \"Confidence too low\"\n",
    "    \n",
    "    # Personalize reply (LLM substitution)\n",
    "    reply_body = substitute_template(\n",
    "        template.body,\n",
    "        context={\n",
    "            'sender_name': job.email_from_name,\n",
    "            'subject': job.email_subject,\n",
    "            'date': job.email_date\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Send via Gmail API or SMTP\n",
    "    connector = EmailConnectorFactory.create(\n",
    "        provider=user.email_accounts[0].provider,\n",
    "        token=decrypt_token(user.email_accounts[0].encrypted_token)\n",
    "    )\n",
    "    \n",
    "    message_id = connector.send_reply(\n",
    "        to=job.email_from,\n",
    "        subject=f\"Re: {job.email_subject}\",\n",
    "        body=reply_body\n",
    "    )\n",
    "    \n",
    "    # Audit\n",
    "    AuditLog.create(\n",
    "        user_id=user.id,\n",
    "        action='auto_reply_sent',\n",
    "        resource_id=email_job_id,\n",
    "        details=encrypt_audit_data({\n",
    "            'template_id': template_id,\n",
    "            'confidence_score': job.classifier_confidence,\n",
    "            'message_id': message_id,\n",
    "            'recipient': job.email_from\n",
    "        })\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data-Analysis Job Pipeline\n",
    "\n",
    "```\n",
    "User uploads CSV or points to data source\n",
    "  ↓\n",
    "Dashboard: Data preview + analysis options\n",
    "  ↓\n",
    "User selects analysis type (summary stats, trend, forecast)\n",
    "  ↓\n",
    "Create \"DataAnalysisJob\" in Celery queue\n",
    "  ↓\n",
    "[Worker] Fetch data → Validate schema → Run analysis script\n",
    "  ↓\n",
    "[LLM] Summarize findings + insights\n",
    "  ↓\n",
    "[Storage] Save report (PDF/HTML) to S3\n",
    "  ↓\n",
    "Return summary + download link to user\n",
    "  ↓\n",
    "Log job metadata (source, record count, analysis time, cost)\n",
    "```\n",
    "\n",
    "### Data Analysis Job Example\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/worker/tasks/data_analysis.py\n",
    "\n",
    "from celery import shared_task\n",
    "from backend.connectors.data import DataConnectorFactory\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "@shared_task\n",
    "def run_data_analysis_job(job_id: int):\n",
    "    job = DataAnalysisJob.query.get(job_id)\n",
    "    user = job.user\n",
    "    \n",
    "    try:\n",
    "        job.status = 'running'\n",
    "        job.save()\n",
    "        \n",
    "        # Fetch data\n",
    "        connector = DataConnectorFactory.create(job.source_type)\n",
    "        df = connector.fetch(job.source_path)\n",
    "        \n",
    "        # Basic analysis\n",
    "        analysis_result = {\n",
    "            'record_count': len(df),\n",
    "            'columns': list(df.columns),\n",
    "            'dtypes': df.dtypes.to_dict(),\n",
    "            'summary_stats': df.describe().to_dict(),\n",
    "            'null_counts': df.isnull().sum().to_dict(),\n",
    "        }\n",
    "        \n",
    "        # Generate LLM summary\n",
    "        summary_prompt = f\"\"\"\n",
    "        Analyze this dataset:\n",
    "        - Records: {analysis_result['record_count']}\n",
    "        - Columns: {', '.join(analysis_result['columns'])}\n",
    "        - Summary stats: {json.dumps(analysis_result['summary_stats'], default=str)}\n",
    "        \n",
    "        Provide 2-3 key insights in plain English.\n",
    "        \"\"\"\n",
    "        \n",
    "        llm_summary = call_llm(summary_prompt)\n",
    "        \n",
    "        # Save report to S3\n",
    "        report_html = f\"\"\"\n",
    "        <html>\n",
    "            <h1>Data Analysis Report</h1>\n",
    "            <p>Generated: {datetime.now()}</p>\n",
    "            <h2>Summary</h2>\n",
    "            <p>{llm_summary}</p>\n",
    "            <h2>Statistics</h2>\n",
    "            <pre>{json.dumps(analysis_result, indent=2, default=str)}</pre>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        report_key = f\"reports/{user.id}/{job.id}/report.html\"\n",
    "        s3_client.put_object(Bucket='reports-bucket', Key=report_key, Body=report_html)\n",
    "        \n",
    "        job.status = 'completed'\n",
    "        job.result_summary = llm_summary\n",
    "        job.report_url = f\"s3://reports-bucket/{report_key}\"\n",
    "        job.save()\n",
    "        \n",
    "        # Audit\n",
    "        AuditLog.create(\n",
    "            user_id=user.id,\n",
    "            action='data_analysis_completed',\n",
    "            resource_id=job_id,\n",
    "            details={'record_count': len(df), 'summary': llm_summary}\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        job.status = 'failed'\n",
    "        job.error_message = str(e)\n",
    "        job.save()\n",
    "        raise\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9b1cc",
   "metadata": {},
   "source": [
    "# Section 4: Modularity & Extension Points\n",
    "\n",
    "## Connector Interface Pattern\n",
    "\n",
    "The system uses an **adapter pattern** to allow easy addition of new email and data connectors.\n",
    "\n",
    "### Base Connector Classes\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/connectors/base.py\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "@dataclass\n",
    "class Email:\n",
    "    message_id: str\n",
    "    from_address: str\n",
    "    subject: str\n",
    "    body: str\n",
    "    received_at: datetime\n",
    "    attachments: List[Dict[str, Any]]\n",
    "\n",
    "class BaseEmailConnector(ABC):\n",
    "    \"\"\"All email adapters implement this interface.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def authenticate(self, token: str) -> bool:\n",
    "        \"\"\"Validate token and establish connection.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fetch_emails(self, limit: int = 10) -> List[Email]:\n",
    "        \"\"\"Fetch unread emails.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def send_email(self, to: str, subject: str, body: str) -> str:\n",
    "        \"\"\"Send an email, return message_id.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def mark_as_read(self, message_id: str) -> bool:\n",
    "        \"\"\"Mark email as read.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def apply_label(self, message_id: str, label: str) -> bool:\n",
    "        \"\"\"Apply a label/folder to email.\"\"\"\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class DataSource:\n",
    "    source_type: str\n",
    "    df: pd.DataFrame\n",
    "    schema: Dict[str, str]\n",
    "    record_count: int\n",
    "\n",
    "class BaseDataConnector(ABC):\n",
    "    \"\"\"All data source adapters implement this interface.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def validate_credentials(self, credentials: Dict) -> bool:\n",
    "        \"\"\"Check if credentials are valid.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fetch(self, source_path: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Fetch data from source.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def schema(self, source_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Return column names and types.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "### Concrete Adapter Examples\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/connectors/email_adapters.py\n",
    "\n",
    "class GmailConnector(BaseEmailConnector):\n",
    "    \"\"\"Gmail API adapter.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.service = None\n",
    "    \n",
    "    def authenticate(self, token: str) -> bool:\n",
    "        from google.oauth2.credentials import Credentials\n",
    "        creds = Credentials.from_authorized_user_info(json.loads(token))\n",
    "        self.service = build('gmail', 'v1', credentials=creds)\n",
    "        return True\n",
    "    \n",
    "    def fetch_emails(self, limit: int = 10) -> List[Email]:\n",
    "        results = self.service.users().messages().list(\n",
    "            userId='me', q='is:unread', maxResults=limit\n",
    "        ).execute()\n",
    "        emails = []\n",
    "        for msg in results.get('messages', []):\n",
    "            full = self.service.users().messages().get(\n",
    "                userId='me', id=msg['id'], format='full'\n",
    "            ).execute()\n",
    "            headers = {h['name']: h['value'] for h in full['payload']['headers']}\n",
    "            body_data = full['payload'].get('body', {}).get('data', '')\n",
    "            emails.append(Email(\n",
    "                message_id=msg['id'],\n",
    "                from_address=headers.get('From', ''),\n",
    "                subject=headers.get('Subject', ''),\n",
    "                body=base64.b64decode(body_data).decode('utf-8') if body_data else '',\n",
    "                received_at=datetime.fromtimestamp(int(full['internalDate']) / 1000),\n",
    "                attachments=[]\n",
    "            ))\n",
    "        return emails\n",
    "\n",
    "class OutlookConnector(BaseEmailConnector):\n",
    "    \"\"\"Outlook/Office365 API adapter.\"\"\"\n",
    "    \n",
    "    def authenticate(self, token: str) -> bool:\n",
    "        self.token = json.loads(token)\n",
    "        self.graph_client = build('outlook', 'v1.0', \n",
    "                                   http_auth=BearerAuth(self.token['access_token']))\n",
    "        return True\n",
    "    \n",
    "    def fetch_emails(self, limit: int = 10) -> List[Email]:\n",
    "        messages = self.graph_client.me.messages.request().top(limit).get()\n",
    "        emails = []\n",
    "        for msg in messages['value']:\n",
    "            emails.append(Email(\n",
    "                message_id=msg['id'],\n",
    "                from_address=msg['from']['emailAddress']['address'],\n",
    "                subject=msg['subject'],\n",
    "                body=msg['bodyPreview'] or '',\n",
    "                received_at=datetime.fromisoformat(msg['receivedDateTime']),\n",
    "                attachments=[]\n",
    "            ))\n",
    "        return emails\n",
    "\n",
    "class IMAPConnector(BaseEmailConnector):\n",
    "    \"\"\"Generic IMAP adapter for any provider.\"\"\"\n",
    "    \n",
    "    def authenticate(self, token: str) -> bool:\n",
    "        creds = json.loads(token)\n",
    "        self.conn = imaplib.IMAP4_SSL(creds['imap_server'])\n",
    "        self.conn.login(creds['email'], creds['password'])\n",
    "        return True\n",
    "    \n",
    "    def fetch_emails(self, limit: int = 10) -> List[Email]:\n",
    "        self.conn.select('INBOX')\n",
    "        _, data = self.conn.search(None, 'UNSEEN')\n",
    "        email_ids = data[0].split()[-limit:]\n",
    "        emails = []\n",
    "        for email_id in email_ids:\n",
    "            _, msg_data = self.conn.fetch(email_id, '(RFC822)')\n",
    "            msg = email.message_from_bytes(msg_data[0][1])\n",
    "            emails.append(Email(\n",
    "                message_id=email_id.decode(),\n",
    "                from_address=msg.get('From', ''),\n",
    "                subject=msg.get('Subject', ''),\n",
    "                body=msg.get_payload(decode=True).decode('utf-8'),\n",
    "                received_at=parsedate_to_datetime(msg.get('Date')),\n",
    "                attachments=[]\n",
    "            ))\n",
    "        return emails\n",
    "\n",
    "# Data connectors...\n",
    "class CSVConnector(BaseDataConnector):\n",
    "    \"\"\"CSV file connector (local or URL).\"\"\"\n",
    "    \n",
    "    def fetch(self, source_path: str, **kwargs) -> pd.DataFrame:\n",
    "        return pd.read_csv(source_path)\n",
    "\n",
    "class GoogleSheetsConnector(BaseDataConnector):\n",
    "    \"\"\"Google Sheets connector.\"\"\"\n",
    "    \n",
    "    def fetch(self, source_path: str, **kwargs) -> pd.DataFrame:\n",
    "        # Parse source_path as spreadsheet_id/sheet_name\n",
    "        service = build('sheets', 'v4', credentials=kwargs.get('creds'))\n",
    "        result = service.spreadsheets().values().get(range=source_path).execute()\n",
    "        values = result.get('values', [])\n",
    "        return pd.DataFrame(values[1:], columns=values[0])\n",
    "\n",
    "class S3Connector(BaseDataConnector):\n",
    "    \"\"\"AWS S3 connector.\"\"\"\n",
    "    \n",
    "    def fetch(self, source_path: str, **kwargs) -> pd.DataFrame:\n",
    "        # source_path = \"s3://bucket/key.csv\"\n",
    "        return pd.read_csv(source_path)\n",
    "\n",
    "class PostgresConnector(BaseDataConnector):\n",
    "    \"\"\"SQL database connector.\"\"\"\n",
    "    \n",
    "    def fetch(self, source_path: str, **kwargs) -> pd.DataFrame:\n",
    "        # source_path = \"SELECT * FROM table\"\n",
    "        engine = create_engine(kwargs.get('connection_string'))\n",
    "        return pd.read_sql(source_path, engine)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Connector Factory & Registration\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/connectors/factory.py\n",
    "\n",
    "class EmailConnectorFactory:\n",
    "    _adapters = {}\n",
    "    \n",
    "    @classmethod\n",
    "    def register(cls, provider: str, adapter_class):\n",
    "        cls._adapters[provider] = adapter_class\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, provider: str, **kwargs) -> BaseEmailConnector:\n",
    "        adapter = cls._adapters.get(provider.lower())\n",
    "        if not adapter:\n",
    "            raise ValueError(f\"Unknown email provider: {provider}\")\n",
    "        return adapter(**kwargs)\n",
    "\n",
    "# Register adapters\n",
    "EmailConnectorFactory.register('gmail', GmailConnector)\n",
    "EmailConnectorFactory.register('outlook', OutlookConnector)\n",
    "EmailConnectorFactory.register('imap', IMAPConnector)\n",
    "\n",
    "# Usage in task:\n",
    "# connector = EmailConnectorFactory.create('gmail', token=user_token)\n",
    "# emails = connector.fetch_emails()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task Definition Serialization\n",
    "\n",
    "Tasks are defined as JSON metadata + handlers, making them pluggable.\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/models/task.py\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class TaskType(str, Enum):\n",
    "    EMAIL_PROCESS = \"email_process\"\n",
    "    AUTO_REPLY = \"auto_reply\"\n",
    "    DATA_ANALYSIS = \"data_analysis\"\n",
    "    FOLLOW_UP = \"follow_up\"\n",
    "\n",
    "class Task(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\n",
    "    task_type = db.Column(db.String(50))\n",
    "    status = db.Column(db.String(20))  # 'pending', 'running', 'completed', 'failed'\n",
    "    celery_task_id = db.Column(db.String(255))\n",
    "    \n",
    "    # JSON serialization of parameters\n",
    "    params = db.Column(db.JSON)  # e.g., {'email_id': 123, 'rule_id': 5}\n",
    "    result = db.Column(db.JSON)  # Output data\n",
    "    error_message = db.Column(db.Text)\n",
    "    \n",
    "    created_at = db.Column(db.DateTime, default=datetime.utcnow)\n",
    "    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n",
    "\n",
    "# Example:\n",
    "# task = Task(\n",
    "#     user_id=1,\n",
    "#     task_type='data_analysis',\n",
    "#     params={'dataset_url': 's3://...', 'analysis_type': 'summary'},\n",
    "#     status='pending'\n",
    "# )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Rules Engine (YAML/JSON DSL)\n",
    "\n",
    "Users define rules in YAML; the engine evaluates them against emails.\n",
    "\n",
    "```yaml\n",
    "# Example rule (stored in DB as JSON or YAML file)\n",
    "rules:\n",
    "  - id: urgent_support_alerts\n",
    "    name: \"Flag urgent support emails\"\n",
    "    trigger: \"email_received\"\n",
    "    conditions:\n",
    "      - field: \"subject\"\n",
    "        operator: \"contains\"\n",
    "        value: [\"urgent\", \"critical\", \"error\"]\n",
    "      - field: \"from_domain\"\n",
    "        operator: \"in\"\n",
    "        value: [\"support@company.com\"]\n",
    "    actions:\n",
    "      - type: \"apply_label\"\n",
    "        label: \"urgent\"\n",
    "      - type: \"flag\"\n",
    "        priority: \"high\"\n",
    "  \n",
    "  - id: auto_reply_newsletters\n",
    "    name: \"Auto-reply to newsletters\"\n",
    "    trigger: \"email_received\"\n",
    "    conditions:\n",
    "      - field: \"subject\"\n",
    "        operator: \"matches_regex\"\n",
    "        value: \"newsletter|digest|announcement\"\n",
    "      - field: \"similarity\"  # Vector DB similarity\n",
    "        operator: \"gt\"\n",
    "        value: 0.8\n",
    "    actions:\n",
    "      - type: \"auto_reply\"\n",
    "        template_id: 5\n",
    "        requires_approval: true  # Must be approved in UI\n",
    "```\n",
    "\n",
    "### Rules Engine Evaluator\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/engine/rules_engine.py\n",
    "\n",
    "import yaml\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class RulesEngine:\n",
    "    def __init__(self, vector_store=None):\n",
    "        self.vector_store = vector_store\n",
    "    \n",
    "    def evaluate_rules(self, email: Email, user_id: int) -> List[Dict]:\n",
    "        \"\"\"Evaluate all user rules against an email. Return matching rules.\"\"\"\n",
    "        rules = Rule.query.filter_by(user_id=user_id, enabled=True).all()\n",
    "        matched_rules = []\n",
    "        \n",
    "        for rule in rules:\n",
    "            rule_config = yaml.safe_load(rule.config)\n",
    "            if self._evaluate_conditions(email, rule_config['conditions']):\n",
    "                matched_rules.append(rule_config)\n",
    "        \n",
    "        return matched_rules\n",
    "    \n",
    "    def _evaluate_conditions(self, email: Email, conditions: List[Dict]) -> bool:\n",
    "        \"\"\"All conditions must pass (AND logic).\"\"\"\n",
    "        for condition in conditions:\n",
    "            if not self._evaluate_condition(email, condition):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _evaluate_condition(self, email: Email, condition: Dict) -> bool:\n",
    "        field = condition.get('field')\n",
    "        operator = condition.get('operator')\n",
    "        value = condition.get('value')\n",
    "        \n",
    "        if field == 'subject':\n",
    "            field_value = email.subject.lower()\n",
    "        elif field == 'body':\n",
    "            field_value = email.body.lower()\n",
    "        elif field == 'from_domain':\n",
    "            field_value = email.from_address.split('@')[1].lower()\n",
    "        elif field == 'similarity':\n",
    "            # Query vector store\n",
    "            embedding = embed_text(email.subject + ' ' + email.body)\n",
    "            similarity = self.vector_store.query_similarity(embedding, top_k=1)[0]['score']\n",
    "            field_value = similarity\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        if operator == 'contains':\n",
    "            return any(v in field_value for v in value)\n",
    "        elif operator == 'matches_regex':\n",
    "            return any(re.search(v, field_value) for v in value)\n",
    "        elif operator == 'in':\n",
    "            return field_value in value\n",
    "        elif operator == 'gt':\n",
    "            return float(field_value) > float(value)\n",
    "        else:\n",
    "            return False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to Add a New Automation\n",
    "\n",
    "### Example: Add a \"Invoice Categorizer\" Task\n",
    "\n",
    "1. **Define the new connector** (if needed):\n",
    "   ```python\n",
    "   class InvoiceSourceConnector(BaseDataConnector):\n",
    "       def fetch(self, source_path: str, **kwargs) -> pd.DataFrame:\n",
    "           # Fetch invoices from accounting software API\n",
    "           pass\n",
    "   ```\n",
    "\n",
    "2. **Create a Celery task** in `backend/worker/tasks/invoice_categorizer.py`:\n",
    "   ```python\n",
    "   @shared_task\n",
    "   def categorize_invoices(job_id: int):\n",
    "       job = DataAnalysisJob.query.get(job_id)\n",
    "       invoices_df = fetch_invoices()\n",
    "       categories = classify_invoices(invoices_df)\n",
    "       save_results(job_id, categories)\n",
    "   ```\n",
    "\n",
    "3. **Add a task type** to `TaskType` enum.\n",
    "\n",
    "4. **Create API endpoint** in `backend/api/routes/jobs.py`:\n",
    "   ```python\n",
    "   @router.post(\"/jobs/categorize-invoices\")\n",
    "   def launch_invoice_categorizer(request: InvoiceCategorizerRequest):\n",
    "       job = DataAnalysisJob(task_type='invoice_categorizer', params=request.dict())\n",
    "       job.save()\n",
    "       categorize_invoices.delay(job.id)\n",
    "       return {'job_id': job.id}\n",
    "   ```\n",
    "\n",
    "5. **Add dashboard page** to trigger and monitor the job.\n",
    "\n",
    "**No changes needed to core orchestration!**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6f114",
   "metadata": {},
   "source": [
    "# Section 5: Development Phases & Roadmap\n",
    "\n",
    "## Phase A: MVP Scoping & Repo Init\n",
    "\n",
    "**Goal**: Set up skeleton repo, basic auth, email linking, and simplified inbox processor.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Init repo + Docker Compose + virtual environment**\n",
    "   - Deliverables: `Dockerfile`, `docker-compose.yml`, `.env.example`, `pyproject.toml` or `requirements.txt`\n",
    "   - Key files: `backend/main.py`, `worker/celery_app.py`, `frontend/pages/index.tsx`\n",
    "   - Success: `docker-compose up` starts all services without errors\n",
    "\n",
    "2. **Implement user auth (OAuth2 + session)**\n",
    "   - Deliverables: `backend/api/auth.py`, OAuth endpoints\n",
    "   - Success: Can register, login, receive session token\n",
    "\n",
    "3. **Implement email account linking (OAuth flow)**\n",
    "   - Deliverables: Gmail/Outlook OAuth endpoints, token encryption, DB schema\n",
    "   - Success: Can link Gmail account, token stored encrypted in DB\n",
    "\n",
    "4. **Fetch & classify inbox (no auto-reply)**\n",
    "   - Deliverables: Email connector, classifier chain (LangChain), FAISS index\n",
    "   - Success: Can see 10 unread emails labeled with category (work, personal, spam)\n",
    "\n",
    "5. **CSV upload & basic analysis**\n",
    "   - Deliverables: Data connector, basic stats worker task, report generation\n",
    "   - Success: Upload CSV → see summary stats + LLM-generated insights\n",
    "\n",
    "### Expected Output:\n",
    "\n",
    "```\n",
    "project-root/\n",
    "├── backend/\n",
    "│   ├── main.py (FastAPI app)\n",
    "│   ├── api/\n",
    "│   │   ├── auth.py\n",
    "│   │   ├── email.py\n",
    "│   │   └── jobs.py\n",
    "│   ├── connectors/\n",
    "│   │   ├── base.py\n",
    "│   │   └── email_adapters.py\n",
    "│   ├── models.py (Postgres schema)\n",
    "│   └── security/\n",
    "│       └── encryption.py\n",
    "├── worker/\n",
    "│   ├── celery_app.py\n",
    "│   └── tasks/\n",
    "│       ├── email_processor.py\n",
    "│       └── data_analysis.py\n",
    "├── frontend/\n",
    "│   ├── pages/\n",
    "│   │   ├── index.tsx\n",
    "│   │   ├── inbox.tsx\n",
    "│   │   └── analysis.tsx\n",
    "│   └── components/\n",
    "├── Dockerfile\n",
    "├── docker-compose.yml\n",
    "├── .env.example\n",
    "├── README.md\n",
    "├── requirements.txt\n",
    "└── pyproject.toml\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Phase B: Core Features & Dashboard\n",
    "\n",
    "**Goal**: Add scheduler, pre-approved replies, vector store, and feature-complete dashboard.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Implement Celery task scheduling + retry logic**\n",
    "   - Deliverables: Task models, Celery configuration, task lifecycle\n",
    "   - Success: Tasks retried 3 times on failure, visible in logs\n",
    "\n",
    "2. **Implement pre-approved reply workflow**\n",
    "   - Deliverables: Template manager API, approval UI, auto-reply execution with safety checks\n",
    "   - Success: Approve template, send auto-reply on rule match, verify delivery\n",
    "\n",
    "3. **Add FAISS vector store + RAG retriever**\n",
    "   - Deliverables: FAISS index builder, retrieval chain, integration with classifier\n",
    "   - Success: Query similar emails + rules, retrieve context for LLM\n",
    "\n",
    "4. **Build dashboard pages**\n",
    "   - Deliverables: Inbox preview, template manager, job launcher, logs viewer\n",
    "   - Success: View emails, approve templates, launch jobs, see execution logs\n",
    "\n",
    "5. **Rules engine (YAML/JSON DSL)**\n",
    "   - Deliverables: Rule model, evaluator, UI for rule builder\n",
    "   - Success: Create rule, email matches rule, action executed\n",
    "\n",
    "### Expected Output:\n",
    "\n",
    "- Email classification confidence scores visible in UI\n",
    "- Pre-approved replies sent with audit trail\n",
    "- Dashboard shows job status in real-time\n",
    "- Rules engine supports regex + similarity matching\n",
    "\n",
    "---\n",
    "\n",
    "## Phase C: Hardening & Deployment\n",
    "\n",
    "**Goal**: Add security, testing, monitoring, and Docker Compose deployment.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Token encryption + secrets management**\n",
    "   - Deliverables: AES-KMS encryption utility, .env-based config, Vault/AWS Secrets integration stub\n",
    "   - Success: Tokens encrypted at rest, never logged\n",
    "\n",
    "2. **Audit logging for all critical actions**\n",
    "   - Deliverables: Audit table schema, middleware to log approvals + auto-replies\n",
    "   - Success: Audit log shows who approved template, when, confidence score, recipient\n",
    "\n",
    "3. **Unit & integration tests**\n",
    "   - Deliverables: `tests/test_email_processor.py`, `tests/test_rules_engine.py`, etc.\n",
    "   - Success: 80%+ code coverage, all tests pass\n",
    "\n",
    "4. **Observability (Sentry + basic metrics)**\n",
    "   - Deliverables: Sentry integration, Prometheus metrics exports\n",
    "   - Success: Errors logged to Sentry, job duration tracked\n",
    "\n",
    "5. **Docker Compose + CI pipeline**\n",
    "   - Deliverables: Fully working `docker-compose.yml`, GitHub Actions workflow\n",
    "   - Success: Push to main → build + test + push Docker image\n",
    "\n",
    "6. **Acceptance test harness**\n",
    "   - Deliverables: Script to test email linking, batch processing, scheduling, reporting\n",
    "   - Success: Run script end-to-end, all checks pass\n",
    "\n",
    "### Expected Output:\n",
    "\n",
    "- Production-ready Docker images\n",
    "- CI/CD pipeline with automated tests\n",
    "- Full audit trail of actions\n",
    "- Monitoring + error tracking set up\n",
    "\n",
    "---\n",
    "\n",
    "## Phase D: Optional Scaling & Enterprise\n",
    "\n",
    "**Goal**: Move to managed services, add k8s, RBAC, advanced connectors.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Migrate FAISS → Pinecone**\n",
    "   - Deliverables: Pinecone client, embedding sync job\n",
    "   - Success: Queries work against Pinecone API, latency < 500ms\n",
    "\n",
    "2. **Migrate Celery → Temporal** (optional)\n",
    "   - Deliverables: Temporal workflow definitions, migration guide\n",
    "   - Success: Jobs execute on Temporal, durable execution guaranteed\n",
    "\n",
    "3. **Setup Kubernetes**\n",
    "   - Deliverables: Helm charts, namespaces, resource limits\n",
    "   - Success: Deploy on k8s cluster, auto-scaling works\n",
    "\n",
    "4. **Role-based access control (RBAC)**\n",
    "   - Deliverables: Role model, permission checks in API\n",
    "   - Success: Admin, approver, user roles with appropriate permissions\n",
    "\n",
    "5. **Advanced connectors**\n",
    "   - Deliverables: BigQuery, Salesforce, Slack adapters\n",
    "   - Success: Connect to multiple data sources seamlessly\n",
    "\n",
    "---\n",
    "\n",
    "## Integration Points Between Phases\n",
    "\n",
    "| Phase | Depends On | Enables |\n",
    "|-------|-----------|---------|\n",
    "| A | – | B, testing infrastructure |\n",
    "| B | A | C, advanced features |\n",
    "| C | A, B | D, production deployment |\n",
    "| D | C | Enterprise scaling |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e06ed",
   "metadata": {},
   "source": [
    "# Section 6: Cost & Infrastructure Planning\n",
    "\n",
    "## Infrastructure Decision Matrix\n",
    "\n",
    "### Compute Options\n",
    "\n",
    "| Option | Cost/Month | Pros | Cons | Recommendation |\n",
    "|--------|-----------|------|------|---|\n",
    "| **DigitalOcean Droplet (6GB) + managed DB** | $12 + $15 | Full control, cheap, straightforward | Manual scaling, limited SLA | **Best for MVP** |\n",
    "| **Render.com (Free tier)** | $0–$30 | Easy deploy, auto-scaling included | Cold starts, limited free tier | Good for testing |\n",
    "| **AWS ECS + RDS** | $30–$100 | Managed, auto-scaling, reliability | Complex setup, multi-service | Scale up here |\n",
    "| **Railway** | $5–$50 | Simple, deploy with Git, managed | Limited customization | Good alternative to Render |\n",
    "| **Kubernetes (self-hosted)** | $50–$200 | Full control, good scaling | High ops overhead | After MVP |\n",
    "\n",
    "**Recommendation**: Start with **DigitalOcean Droplet** (all services on $12 droplet) → migrate to **Render** or **Railway** when needing auto-scaling.\n",
    "\n",
    "---\n",
    "\n",
    "### Storage & Database Options\n",
    "\n",
    "| Component | MVP Solution | Cost/Month | Scale-Up Solution | Cost/Month |\n",
    "|-----------|---|---|---|---|\n",
    "| **Database** | Postgres (managed DO) | $15 | AWS RDS Multi-AZ | $100+ |\n",
    "| **Object Storage** | DO Spaces (250 GB) | $5 | AWS S3 (pay-per-use) | $1–$50 |\n",
    "| **Vector DB** | FAISS (in-memory) | $0 | Pinecone (managed) | $20–$100 |\n",
    "| **Cache/Queue** | Redis (managed DO) | $15 | Redis Cluster (AWS) | $40–$100 |\n",
    "\n",
    "**Total MVP Storage Cost**: ~$35/month\n",
    "\n",
    "---\n",
    "\n",
    "### LLM & AI Cost Estimates\n",
    "\n",
    "#### OpenAI Pricing (current)\n",
    "\n",
    "| Model | Input | Output | Use Case |\n",
    "|-------|-------|--------|----------|\n",
    "| GPT-3.5 Turbo | $0.0005/1K | $0.0015/1K | Email classification, simple summaries |\n",
    "| GPT-4 Turbo | $0.01/1K | $0.03/1K | Complex analysis, high accuracy |\n",
    "| Embeddings (3-small) | $0.02/1M | – | Vector embeddings for FAISS |\n",
    "\n",
    "#### Cost Projections\n",
    "\n",
    "**Scenario 1: 100 emails/day + 5 data analysis jobs/day**\n",
    "\n",
    "- Email classification: 100 × 150 tokens × $0.0005 = $0.075/day = $2.25/month\n",
    "- Embeddings: 100 × 8 tokens × $0.00002 = $0.016/day = $0.48/month\n",
    "- Data analysis summary: 5 × 500 tokens × $0.001 = $0.0025/day = $0.075/month\n",
    "- **Total LLM**: ~$2.80/month\n",
    "\n",
    "**Scenario 2: 1,000 emails/day + 50 data analysis jobs/day (10x scale)**\n",
    "\n",
    "- **Total LLM**: ~$28/month\n",
    "\n",
    "**Cost Savings**:\n",
    "- **Caching**: Prompt caching saves 50–90% on repeated queries\n",
    "- **Batch processing**: Run off-peak for 50% discount with OpenAI Batch API\n",
    "- **Local LLM**: Switch to LLaMA 2 on-prem after MVP ($0 per call, +$200 infra)\n",
    "\n",
    "---\n",
    "\n",
    "## Monthly Cost Breakdown\n",
    "\n",
    "### MVP (100 emails/day)\n",
    "\n",
    "| Component | Cost |\n",
    "|-----------|------|\n",
    "| Compute (Droplet) | $12 |\n",
    "| Database (Postgres) | $15 |\n",
    "| Cache (Redis) | $15 |\n",
    "| Storage (DO Spaces) | $5 |\n",
    "| LLM (OpenAI) | $3 |\n",
    "| Monitoring (Sentry free) | $0 |\n",
    "| Domain + SSL | $0 |\n",
    "| **TOTAL** | **$50/month** |\n",
    "\n",
    "### Scale (1,000 emails/day, auto-scaling)\n",
    "\n",
    "| Component | Cost |\n",
    "|-----------|------|\n",
    "| Compute (Render auto-scale) | $50 |\n",
    "| Database (AWS RDS) | $40 |\n",
    "| Cache (AWS ElastiCache) | $50 |\n",
    "| Storage (S3) | $20 |\n",
    "| LLM (OpenAI) | $28 |\n",
    "| Monitoring (Sentry + Datadog) | $50 |\n",
    "| Kubernetes (optional) | $0 (if managed) |\n",
    "| **TOTAL** | **$238/month** |\n",
    "\n",
    "---\n",
    "\n",
    "## Infrastructure Migration Strategy\n",
    "\n",
    "```\n",
    "Month 1-2: MVP (DigitalOcean)\n",
    "  ├── Backend + Frontend: Droplet ($12)\n",
    "  ├── Database: DO Postgres ($15)\n",
    "  ├── Cache: DO Redis ($15)\n",
    "  └── Object storage: DO Spaces ($5)\n",
    "\n",
    "Month 3-6: Scale Compute (Render)\n",
    "  ├── Backend + Frontend: Render auto-scale ($30–$50)\n",
    "  ├── Database: AWS RDS Managed ($40)\n",
    "  ├── Cache: AWS ElastiCache ($50)\n",
    "  └── Object storage: AWS S3 ($20)\n",
    "\n",
    "Month 6+: Enterprise (Kubernetes)\n",
    "  ├── Backend + Frontend: GKE / EKS cluster ($100+)\n",
    "  ├── Database: AWS RDS Multi-AZ ($100+)\n",
    "  ├── Cache: Redis Cluster ($50+)\n",
    "  ├── Vector DB: Pinecone Managed ($50+)\n",
    "  └── Task Scheduler: Temporal Cloud ($100+)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Optimization Strategies\n",
    "\n",
    "### Immediate (MVP Phase)\n",
    "\n",
    "| Strategy | Savings | Effort |\n",
    "|----------|---------|--------|\n",
    "| Use FAISS instead of Pinecone | $20–$100/month | Low |\n",
    "| Cache LLM outputs | 50–90% on tokens | Medium |\n",
    "| Batch email processing | 20% on compute | Low |\n",
    "| Self-host Postgres + Redis on Droplet | $30/month | Medium |\n",
    "| OpenAI Batch API for off-peak | 50% discount | Medium |\n",
    "\n",
    "### Medium-term (Phase B–C)\n",
    "\n",
    "| Strategy | Savings | Effort |\n",
    "|----------|---------|--------|\n",
    "| Move to local LLaMA 2 | $28+/month → $0 | High |\n",
    "| Compress email storage | 30% storage savings | Low |\n",
    "| Use Celery task grouping | 20% compute | Medium |\n",
    "| Archive old logs to S3 Glacier | 80% storage cost | Low |\n",
    "\n",
    "### Long-term (Phase D+)\n",
    "\n",
    "| Strategy | Savings | Effort |\n",
    "|----------|---------|--------|\n",
    "| Temporal (durable execution) | Eliminate retries, save compute | High |\n",
    "| Regional Pinecone | Save egress costs | Low |\n",
    "| Reserved instances (AWS/GCP) | 40–70% compute discount | High |\n",
    "| Data residency optimization | Reduce data transfer | Medium |\n",
    "\n",
    "---\n",
    "\n",
    "## ROI & Pricing Model\n",
    "\n",
    "### Revenue Model Options\n",
    "\n",
    "1. **Per-user subscription**: $10–$50/month per user\n",
    "   - MVP cost per user: $0.05/month\n",
    "   - Gross margin: 95%+\n",
    "\n",
    "2. **Per-action billing**: $0.01–$0.10 per email processed or analysis\n",
    "   - Based on LLM token usage\n",
    "   - Shared infrastructure costs\n",
    "\n",
    "3. **Enterprise tier**: $500–$5,000/month\n",
    "   - Dedicated support, advanced connectors, on-prem option\n",
    "\n",
    "### Break-even Analysis\n",
    "\n",
    "**Assume SaaS Model: $20/month per user**\n",
    "\n",
    "- Monthly cost per user: $0.05\n",
    "- Gross margin per user: 99.75%\n",
    "- Break-even: 3 users (covers ~$50 MVP infra)\n",
    "- 10 users: $200 revenue vs $50 cost = $150 profit\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9426ae",
   "metadata": {},
   "source": [
    "# Section 7: Security & Compliance Requirements\n",
    "\n",
    "## OAuth2 Email Account Linking\n",
    "\n",
    "### Flow Diagram\n",
    "\n",
    "```\n",
    "User clicks \"Link Email\" → Backend generates auth URL\n",
    "  ↓\n",
    "User approves scopes on provider (Gmail/Outlook) → Gets authorization code\n",
    "  ↓\n",
    "Backend exchanges code for access token + refresh token\n",
    "  ↓\n",
    "Encrypt tokens with AES-256 + KMS key ID\n",
    "  ↓\n",
    "Store in DB: user_id, provider, encrypted_token, scopes, created_at\n",
    "  ↓\n",
    "User sees \"Account linked successfully\"\n",
    "```\n",
    "\n",
    "### Implementation\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/api/auth.py\n",
    "\n",
    "@router.get(\"/email/oauth-callback/{provider}\")\n",
    "def email_oauth_callback(provider: str, code: str, state: str):\n",
    "    \"\"\"Handle OAuth callback from email provider.\"\"\"\n",
    "    \n",
    "    # Verify state token (CSRF protection)\n",
    "    stored_state = cache.get(f\"oauth_state:{state}\")\n",
    "    if not stored_state:\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid state\")\n",
    "    \n",
    "    # Exchange code for token\n",
    "    if provider.lower() == 'gmail':\n",
    "        token_data = request_gmail_token(code)\n",
    "    elif provider.lower() == 'outlook':\n",
    "        token_data = request_outlook_token(code)\n",
    "    else:\n",
    "        raise HTTPException(status_code=400, detail=\"Unknown provider\")\n",
    "    \n",
    "    # Encrypt token\n",
    "    user = get_current_user()\n",
    "    encrypted_token = encrypt_token(json.dumps(token_data))\n",
    "    \n",
    "    # Store in DB\n",
    "    account = UserEmailAccount(\n",
    "        user_id=user.id,\n",
    "        provider=provider,\n",
    "        email_address=extract_email_from_token(token_data),\n",
    "        encrypted_token=encrypted_token,\n",
    "        scopes=json.dumps(token_data.get('scope', '').split()),\n",
    "        refresh_token_encrypted=encrypt_token(token_data.get('refresh_token', '')),\n",
    "    )\n",
    "    db.session.add(account)\n",
    "    db.session.commit()\n",
    "    \n",
    "    # Log action\n",
    "    AuditLog.create(\n",
    "        user_id=user.id,\n",
    "        action='email_account_linked',\n",
    "        resource_id=account.id,\n",
    "        details={'provider': provider, 'email': account.email_address}\n",
    "    )\n",
    "    \n",
    "    return {'status': 'success', 'email': account.email_address}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Token Encryption at Rest\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/security/encryption.py\n",
    "\n",
    "from cryptography.fernet import Fernet\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import os\n",
    "\n",
    "class TokenEncryptor:\n",
    "    \"\"\"Encrypt/decrypt OAuth tokens with Fernet (AES-128).\"\"\"\n",
    "    \n",
    "    def __init__(self, master_key: str = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            master_key: Base64-encoded master key. If None, load from env or KMS.\n",
    "        \"\"\"\n",
    "        if not master_key:\n",
    "            # In production, retrieve from AWS Secrets Manager / HashiCorp Vault\n",
    "            master_key = os.getenv('ENCRYPTION_MASTER_KEY')\n",
    "        \n",
    "        self.cipher = Fernet(master_key.encode() if isinstance(master_key, str) else master_key)\n",
    "    \n",
    "    def encrypt_token(self, token: str, associated_data: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Encrypt a token.\n",
    "        \n",
    "        Args:\n",
    "            token: Plain text OAuth token\n",
    "            associated_data: Optional user_id or metadata for audit\n",
    "        \n",
    "        Returns:\n",
    "            Base64-encoded encrypted token\n",
    "        \"\"\"\n",
    "        encrypted = self.cipher.encrypt(token.encode())\n",
    "        return encrypted.decode()\n",
    "    \n",
    "    def decrypt_token(self, encrypted_token: str) -> str:\n",
    "        \"\"\"Decrypt a token.\"\"\"\n",
    "        decrypted = self.cipher.decrypt(encrypted_token.encode())\n",
    "        return decrypted.decode()\n",
    "    \n",
    "    def is_valid(self, encrypted_token: str) -> bool:\n",
    "        \"\"\"Check if token is readable (not corrupted).\"\"\"\n",
    "        try:\n",
    "            self.decrypt_token(encrypted_token)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# Usage:\n",
    "# encryptor = TokenEncryptor()\n",
    "# encrypted = encryptor.encrypt_token(oauth_token)\n",
    "# original = encryptor.decrypt_token(encrypted)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Audit Logging for Auto-Replies & Job Actions\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/models.py\n",
    "\n",
    "class AuditLog(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)\n",
    "    action = db.Column(db.String(100), nullable=False)  # 'auto_reply_sent', 'template_approved', etc.\n",
    "    resource_id = db.Column(db.Integer)  # Email ID, template ID, job ID, etc.\n",
    "    resource_type = db.Column(db.String(50))  # 'email', 'template', 'job'\n",
    "    \n",
    "    # Encrypted sensitive data\n",
    "    _details_encrypted = db.Column(db.Text)  # Encrypted JSON\n",
    "    \n",
    "    created_at = db.Column(db.DateTime, default=datetime.utcnow, nullable=False)\n",
    "    ip_address = db.Column(db.String(45))  # IPv4 or IPv6\n",
    "    user_agent = db.Column(db.Text)\n",
    "    \n",
    "    @property\n",
    "    def details(self):\n",
    "        \"\"\"Return decrypted details.\"\"\"\n",
    "        if self._details_encrypted:\n",
    "            return json.loads(decrypt_token(self._details_encrypted))\n",
    "        return {}\n",
    "    \n",
    "    @details.setter\n",
    "    def details(self, value):\n",
    "        \"\"\"Encrypt and store details.\"\"\"\n",
    "        self._details_encrypted = encrypt_token(json.dumps(value))\n",
    "\n",
    "# Audit log entries for auto-reply:\n",
    "# {\n",
    "#     user_id: 123,\n",
    "#     action: 'auto_reply_sent',\n",
    "#     resource_id: 456,  # email_id\n",
    "#     resource_type: 'email',\n",
    "#     details: {\n",
    "#         template_id: 5,\n",
    "#         confidence_score: 0.92,\n",
    "#         recipient: 'user@example.com',\n",
    "#         message_id: 'msg_xyz',\n",
    "#         approved_by: 123  # user_id who approved template\n",
    "#     },\n",
    "#     ip_address: '192.168.1.1',\n",
    "#     user_agent: 'Mozilla/5.0...'\n",
    "# }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-Approved Reply Safety Gates\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/worker/tasks/auto_reply.py\n",
    "\n",
    "class AutoReplyValidator:\n",
    "    \"\"\"Validate that auto-reply is safe to send.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate(email_job, template, user) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validate auto-reply before sending.\n",
    "        \n",
    "        Returns:\n",
    "            (is_valid: bool, reason: str)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Gate 1: Template approval\n",
    "        if not template.approved_for_auto_send:\n",
    "            return False, \"Template not approved for auto-send\"\n",
    "        \n",
    "        # Gate 2: Confidence threshold\n",
    "        MIN_CONFIDENCE = 0.85\n",
    "        if email_job.classifier_confidence < MIN_CONFIDENCE:\n",
    "            return False, f\"Confidence {email_job.classifier_confidence} below threshold {MIN_CONFIDENCE}\"\n",
    "        \n",
    "        # Gate 3: Daily send limit\n",
    "        daily_sent = count_replies_sent_today(user.id)\n",
    "        if daily_sent >= user.settings.max_daily_replies:\n",
    "            return False, f\"Daily limit ({user.settings.max_daily_replies}) reached\"\n",
    "        \n",
    "        # Gate 4: Opt-in flag\n",
    "        if not user.settings.enable_auto_replies:\n",
    "            return False, \"Auto-replies not enabled by user\"\n",
    "        \n",
    "        # Gate 5: Sensitive topics (content-based filter)\n",
    "        SENSITIVE_KEYWORDS = ['refund', 'dispute', 'complaint', 'legal']\n",
    "        if any(kw in email_job.email_subject.lower() for kw in SENSITIVE_KEYWORDS):\n",
    "            return False, \"Email matches sensitive topic pattern\"\n",
    "        \n",
    "        return True, \"All validations passed\"\n",
    "\n",
    "# Usage:\n",
    "# is_valid, reason = AutoReplyValidator.validate(job, template, user)\n",
    "# if not is_valid:\n",
    "#     flag_email_for_human_review(job.id, reason)\n",
    "# else:\n",
    "#     send_auto_reply(job, template)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Role-Based Access Control (RBAC)\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/models.py\n",
    "\n",
    "class Role(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    name = db.Column(db.String(50), unique=True)\n",
    "    description = db.Column(db.Text)\n",
    "    permissions = db.Column(db.JSON)  # List of permission codes\n",
    "\n",
    "class User(db.Model):\n",
    "    # ... other fields ...\n",
    "    role_id = db.Column(db.Integer, db.ForeignKey('role.id'))\n",
    "    role = db.relationship('Role')\n",
    "\n",
    "# Example roles:\n",
    "# {\n",
    "#     'admin': {\n",
    "#         'permissions': [\n",
    "#             'manage_users',\n",
    "#             'view_all_emails',\n",
    "#             'approve_templates',\n",
    "#             'modify_rules',\n",
    "#             'view_audit_logs'\n",
    "#         ]\n",
    "#     },\n",
    "#     'approver': {\n",
    "#         'permissions': [\n",
    "#             'view_own_emails',\n",
    "#             'approve_templates',\n",
    "#             'approve_auto_replies'\n",
    "#         ]\n",
    "#     },\n",
    "#     'user': {\n",
    "#         'permissions': [\n",
    "#             'view_own_emails',\n",
    "#             'create_templates',\n",
    "#             'create_rules',\n",
    "#             'launch_jobs'\n",
    "#         ]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Decorator for permission checks:\n",
    "from functools import wraps\n",
    "\n",
    "def require_permission(permission: str):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            user = get_current_user()\n",
    "            if permission not in user.role.permissions:\n",
    "                raise HTTPException(status_code=403, detail=\"Forbidden\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Usage:\n",
    "# @router.post(\"/templates/{template_id}/approve\")\n",
    "# @require_permission(\"approve_templates\")\n",
    "# def approve_template(template_id: int):\n",
    "#     ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Retention & GDPR Compliance\n",
    "\n",
    "```python\n",
    "# Pseudocode: backend/models.py\n",
    "\n",
    "class UserDataRetentionPolicy(db.Model):\n",
    "    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), primary_key=True)\n",
    "    email_retention_days = db.Column(db.Integer, default=90)  # 90 days\n",
    "    audit_log_retention_days = db.Column(db.Integer, default=365)  # 1 year\n",
    "    reports_retention_days = db.Column(db.Integer, default=30)  # 30 days\n",
    "    auto_delete_enabled = db.Column(db.Boolean, default=True)\n",
    "\n",
    "# Background task (run daily):\n",
    "@shared_task\n",
    "def cleanup_expired_data():\n",
    "    \"\"\"Delete data older than retention period.\"\"\"\n",
    "    \n",
    "    policies = UserDataRetentionPolicy.query.filter_by(auto_delete_enabled=True).all()\n",
    "    \n",
    "    for policy in policies:\n",
    "        user_id = policy.user_id\n",
    "        \n",
    "        # Delete old emails\n",
    "        cutoff_date = datetime.utcnow() - timedelta(days=policy.email_retention_days)\n",
    "        EmailJob.query.filter(\n",
    "            EmailJob.user_id == user_id,\n",
    "            EmailJob.created_at < cutoff_date\n",
    "        ).delete()\n",
    "        \n",
    "        # Delete old audit logs (but keep for compliance if needed)\n",
    "        cutoff_date = datetime.utcnow() - timedelta(days=policy.audit_log_retention_days)\n",
    "        AuditLog.query.filter(\n",
    "            AuditLog.user_id == user_id,\n",
    "            AuditLog.created_at < cutoff_date\n",
    "        ).delete()\n",
    "        \n",
    "        # Delete old reports\n",
    "        cutoff_date = datetime.utcnow() - timedelta(days=policy.reports_retention_days)\n",
    "        DataAnalysisJob.query.filter(\n",
    "            DataAnalysisJob.user_id == user_id,\n",
    "            DataAnalysisJob.completed_at < cutoff_date\n",
    "        ).delete()\n",
    "    \n",
    "    db.session.commit()\n",
    "\n",
    "# GDPR: User can request data export\n",
    "@router.post(\"/user/export-data\")\n",
    "def export_user_data():\n",
    "    \"\"\"Export all user data as JSON.\"\"\"\n",
    "    user = get_current_user()\n",
    "    \n",
    "    data = {\n",
    "        'user': user.to_dict(),\n",
    "        'email_accounts': [acc.to_dict() for acc in user.email_accounts],\n",
    "        'emails': [job.to_dict() for job in user.email_jobs],\n",
    "        'templates': [t.to_dict() for t in user.templates],\n",
    "        'rules': [r.to_dict() for r in user.rules],\n",
    "        'jobs': [j.to_dict() for j in user.data_analysis_jobs],\n",
    "        'audit_logs': [log.to_dict() for log in user.audit_logs],\n",
    "    }\n",
    "    \n",
    "    # Send as JSON file\n",
    "    return FileResponse(\n",
    "        json.dumps(data, indent=2),\n",
    "        media_type='application/json',\n",
    "        filename=f'user_data_export_{user.id}.json'\n",
    "    )\n",
    "\n",
    "# GDPR: User can request deletion\n",
    "@router.post(\"/user/delete-account\")\n",
    "def delete_user_account():\n",
    "    \"\"\"Delete all user data (hard delete after retention period).\"\"\"\n",
    "    user = get_current_user()\n",
    "    \n",
    "    # Schedule deletion after 30-day grace period\n",
    "    user.deletion_requested_at = datetime.utcnow()\n",
    "    db.session.commit()\n",
    "    \n",
    "    # Background task will hard delete after 30 days\n",
    "    return {'status': 'deletion scheduled', 'effective_date': (datetime.utcnow() + timedelta(days=30)).isoformat()}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## TLS & Certificate Management\n",
    "\n",
    "```bash\n",
    "# Using Let's Encrypt + Certbot (automated renewal)\n",
    "\n",
    "# Initial setup:\n",
    "certbot certonly --standalone -d yourdomain.com\n",
    "\n",
    "# Auto-renewal via cron (runs twice daily):\n",
    "0 3,15 * * * certbot renew --quiet\n",
    "\n",
    "# In docker-compose.yml:\n",
    "# volumes:\n",
    "#   - /etc/letsencrypt:/etc/letsencrypt\n",
    "#   - /var/log/letsencrypt:/var/log/letsencrypt\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9156d",
   "metadata": {},
   "source": [
    "# Section 8: Acceptance Testing & Validation\n",
    "\n",
    "## Acceptance Test Cases\n",
    "\n",
    "### Test 1: User Login & OAuth Email Linking\n",
    "\n",
    "**Objective**: Verify secure authentication and email account linking.\n",
    "\n",
    "```python\n",
    "# Pseudocode: tests/test_acceptance_auth.py\n",
    "\n",
    "def test_user_login_and_email_linking():\n",
    "    \"\"\"User can login and link Gmail account.\"\"\"\n",
    "    \n",
    "    # Step 1: Register user\n",
    "    response = client.post('/api/auth/register', json={\n",
    "        'email': 'test@example.com',\n",
    "        'password': 'secure_password'\n",
    "    })\n",
    "    assert response.status_code == 201\n",
    "    user_id = response.json()['user_id']\n",
    "    \n",
    "    # Step 2: Login\n",
    "    response = client.post('/api/auth/login', json={\n",
    "        'email': 'test@example.com',\n",
    "        'password': 'secure_password'\n",
    "    })\n",
    "    assert response.status_code == 200\n",
    "    session_token = response.json()['token']\n",
    "    \n",
    "    # Step 3: Initiate Gmail OAuth\n",
    "    response = client.get(\n",
    "        '/api/email/oauth-authorize',\n",
    "        params={'provider': 'gmail'},\n",
    "        headers={'Authorization': f'Bearer {session_token}'}\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "    oauth_url = response.json()['authorization_url']\n",
    "    \n",
    "    # Step 4: Simulate OAuth callback\n",
    "    response = client.get(\n",
    "        '/api/email/oauth-callback/gmail',\n",
    "        params={\n",
    "            'code': 'mock_auth_code',\n",
    "            'state': 'mock_state'\n",
    "        }\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "    \n",
    "    # Step 5: Verify token encrypted in DB\n",
    "    from backend.models import UserEmailAccount\n",
    "    account = UserEmailAccount.query.filter_by(user_id=user_id).first()\n",
    "    assert account is not None\n",
    "    assert account.provider == 'gmail'\n",
    "    # Token should be encrypted (not plain text)\n",
    "    assert len(account.encrypted_token) > 100\n",
    "    assert 'refresh_token' not in account.encrypted_token\n",
    "    \n",
    "    print(\"✓ Test 1 passed: Login and OAuth linking works\")\n",
    "```\n",
    "\n",
    "### Test 2: Email Batch Processing & Classification\n",
    "\n",
    "**Objective**: Process test emails and verify classification labels.\n",
    "\n",
    "```python\n",
    "def test_email_batch_processing():\n",
    "    \"\"\"Upload test emails, process them, verify labels.\"\"\"\n",
    "    \n",
    "    user = create_test_user()\n",
    "    link_test_email_account(user)\n",
    "    \n",
    "    # Queue email processing task\n",
    "    from backend.worker.tasks.email_processor import process_email\n",
    "    \n",
    "    # Create test email job\n",
    "    test_emails = [\n",
    "        {\n",
    "            'id': '1',\n",
    "            'subject': 'Q3 Financial Report - URGENT',\n",
    "            'from': 'finance@company.com',\n",
    "            'body': 'Please review attached quarterly report...'\n",
    "        },\n",
    "        {\n",
    "            'id': '2',\n",
    "            'subject': 'Happy Birthday from the team!',\n",
    "            'from': 'hr@company.com',\n",
    "            'body': 'We hope you have a great day...'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for email_data in test_emails:\n",
    "        job = EmailJob(\n",
    "            user_id=user.id,\n",
    "            message_id=email_data['id'],\n",
    "            email_subject=email_data['subject'],\n",
    "            email_from=email_data['from'],\n",
    "            email_body=email_data['body'],\n",
    "            status='queued'\n",
    "        )\n",
    "        db.session.add(job)\n",
    "        db.session.commit()\n",
    "        \n",
    "        # Process email\n",
    "        process_email(job.id)\n",
    "    \n",
    "    # Verify results\n",
    "    finance_email = EmailJob.query.filter_by(message_id='1').first()\n",
    "    assert 'financial' in finance_email.applied_labels or 'work' in finance_email.applied_labels\n",
    "    assert finance_email.status == 'processed'\n",
    "    assert finance_email.classifier_confidence > 0.7\n",
    "    \n",
    "    birthday_email = EmailJob.query.filter_by(message_id='2').first()\n",
    "    assert 'personal' in birthday_email.applied_labels or 'greeting' in birthday_email.applied_labels\n",
    "    \n",
    "    print(\"✓ Test 2 passed: Email classification works\")\n",
    "```\n",
    "\n",
    "### Test 3: Pre-Approved Template & Auto-Reply\n",
    "\n",
    "**Objective**: Approve template, send auto-reply, verify delivery.\n",
    "\n",
    "```python\n",
    "def test_auto_reply_workflow():\n",
    "    \"\"\"Approve template, trigger auto-reply, verify sent.\"\"\"\n",
    "    \n",
    "    user = create_test_user()\n",
    "    \n",
    "    # Step 1: Create template\n",
    "    template = Template(\n",
    "        user_id=user.id,\n",
    "        name='Support Auto-Reply',\n",
    "        subject='Thanks for contacting us',\n",
    "        body='Thank you for your email. We will get back to you shortly.\\nBest regards',\n",
    "        approved_for_auto_send=False\n",
    "    )\n",
    "    db.session.add(template)\n",
    "    db.session.commit()\n",
    "    \n",
    "    # Step 2: Approve template via API\n",
    "    response = client.post(\n",
    "        f'/api/templates/{template.id}/approve',\n",
    "        headers={'Authorization': f'Bearer {user_token}'}\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "    \n",
    "    # Verify audit log\n",
    "    audit = AuditLog.query.filter(\n",
    "        AuditLog.user_id == user.id,\n",
    "        AuditLog.action == 'template_approved'\n",
    "    ).first()\n",
    "    assert audit is not None\n",
    "    \n",
    "    # Step 3: Incoming email matches rule → trigger auto-reply\n",
    "    email_job = EmailJob(\n",
    "        user_id=user.id,\n",
    "        message_id='support_001',\n",
    "        email_subject='Question about your service',\n",
    "        email_from='customer@example.com',\n",
    "        email_body='Hi, I have a question...',\n",
    "        classifier_confidence=0.9,\n",
    "        status='processed'\n",
    "    )\n",
    "    db.session.add(email_job)\n",
    "    db.session.commit()\n",
    "    \n",
    "    # Execute auto-reply task\n",
    "    from backend.worker.tasks.auto_reply import send_auto_reply\n",
    "    send_auto_reply(email_job.id, template.id)\n",
    "    \n",
    "    # Verify email was sent (check mock SMTP or Gmail API)\n",
    "    audit_log = AuditLog.query.filter(\n",
    "        AuditLog.action == 'auto_reply_sent',\n",
    "        AuditLog.resource_id == email_job.id\n",
    "    ).first()\n",
    "    assert audit_log is not None\n",
    "    assert 'message_id' in audit_log.details\n",
    "    \n",
    "    print(\"✓ Test 3 passed: Auto-reply workflow works\")\n",
    "```\n",
    "\n",
    "### Test 4: Follow-Up Task Scheduling\n",
    "\n",
    "**Objective**: Create follow-up task from email, confirm scheduled in Celery.\n",
    "\n",
    "```python\n",
    "def test_follow_up_scheduling():\n",
    "    \"\"\"Create follow-up task, verify Celery scheduling.\"\"\"\n",
    "    \n",
    "    user = create_test_user()\n",
    "    email_job = create_test_email(user)\n",
    "    \n",
    "    # Create follow-up task via API\n",
    "    response = client.post(\n",
    "        f'/api/emails/{email_job.id}/create-followup',\n",
    "        json={\n",
    "            'task_type': 'follow_up',\n",
    "            'scheduled_for': (datetime.utcnow() + timedelta(days=1)).isoformat(),\n",
    "            'notes': 'Follow up if no response'\n",
    "        },\n",
    "        headers={'Authorization': f'Bearer {user_token}'}\n",
    "    )\n",
    "    assert response.status_code == 201\n",
    "    task_id = response.json()['task_id']\n",
    "    \n",
    "    # Verify task in DB\n",
    "    task = Task.query.get(task_id)\n",
    "    assert task.status == 'pending'\n",
    "    assert task.task_type == 'follow_up'\n",
    "    \n",
    "    # Verify Celery task ID recorded\n",
    "    assert task.celery_task_id is not None\n",
    "    \n",
    "    print(\"✓ Test 4 passed: Task scheduling works\")\n",
    "```\n",
    "\n",
    "### Test 5: Data-Analysis Job End-to-End\n",
    "\n",
    "**Objective**: Upload CSV, run analysis, verify LLM summary and report.\n",
    "\n",
    "```python\n",
    "def test_data_analysis_job():\n",
    "    \"\"\"Upload CSV, analyze, verify summary and report generation.\"\"\"\n",
    "    \n",
    "    user = create_test_user()\n",
    "    \n",
    "    # Step 1: Upload test CSV\n",
    "    csv_content = \"\"\"date,sales,region,product\n",
    "2024-01-01,1000,North,Product A\n",
    "2024-01-02,1200,North,Product A\n",
    "2024-01-01,800,South,Product B\n",
    "2024-01-02,950,South,Product B\"\"\"\n",
    "    \n",
    "    response = client.post(\n",
    "        '/api/data/upload',\n",
    "        files={'file': ('test.csv', csv_content)},\n",
    "        headers={'Authorization': f'Bearer {user_token}'}\n",
    "    )\n",
    "    assert response.status_code == 201\n",
    "    job_id = response.json()['job_id']\n",
    "    \n",
    "    # Step 2: Launch analysis job\n",
    "    response = client.post(\n",
    "        f'/api/jobs/{job_id}/analyze',\n",
    "        json={'analysis_type': 'summary_stats'},\n",
    "        headers={'Authorization': f'Bearer {user_token}'}\n",
    "    )\n",
    "    assert response.status_code == 202\n",
    "    \n",
    "    # Step 3: Wait for job completion\n",
    "    job = wait_for_job_completion(job_id, timeout=30)\n",
    "    assert job.status == 'completed'\n",
    "    \n",
    "    # Step 4: Verify results\n",
    "    assert job.result_summary is not None\n",
    "    assert len(job.result_summary) > 0\n",
    "    assert 'sales' in job.result_summary.lower() or 'region' in job.result_summary.lower()\n",
    "    \n",
    "    # Step 5: Verify report generated\n",
    "    assert job.report_url is not None\n",
    "    report_response = requests.get(job.report_url)\n",
    "    assert report_response.status_code == 200\n",
    "    assert 'html' in report_response.headers['content-type'].lower()\n",
    "    \n",
    "    print(\"✓ Test 5 passed: Data analysis job works end-to-end\")\n",
    "```\n",
    "\n",
    "### Test 6: Audit Log Verification\n",
    "\n",
    "**Objective**: Verify all critical actions logged with encryption.\n",
    "\n",
    "```python\n",
    "def test_audit_logs():\n",
    "    \"\"\"Verify audit logs capture all critical actions.\"\"\"\n",
    "    \n",
    "    user = create_test_user()\n",
    "    \n",
    "    # Perform actions that should be logged\n",
    "    link_test_email_account(user)  # Logs: email_account_linked\n",
    "    approve_template(user)  # Logs: template_approved\n",
    "    send_auto_reply(user)  # Logs: auto_reply_sent\n",
    "    \n",
    "    # Query audit logs\n",
    "    logs = AuditLog.query.filter_by(user_id=user.id).all()\n",
    "    assert len(logs) >= 3\n",
    "    \n",
    "    # Verify encrypted details\n",
    "    for log in logs:\n",
    "        assert log._details_encrypted is not None\n",
    "        assert len(log._details_encrypted) > 100\n",
    "        # Verify we can decrypt and read\n",
    "        details = log.details\n",
    "        assert isinstance(details, dict)\n",
    "    \n",
    "    print(\"✓ Test 6 passed: Audit logging works with encryption\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Test Harness & Mock Data Generator\n",
    "\n",
    "```python\n",
    "# Pseudocode: tests/harness.py\n",
    "\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class TestDataGenerator:\n",
    "    \"\"\"Generate realistic test data.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_emails(count: int = 10) -> list:\n",
    "        subjects = [\n",
    "            \"Q{} Financial Report\",\n",
    "            \"Team Meeting - {}\",\n",
    "            \"Action Required: {}\",\n",
    "            \"Birthday Reminder\",\n",
    "            \"Project Update\"\n",
    "        ]\n",
    "        from_addresses = [\n",
    "            \"finance@company.com\",\n",
    "            \"hr@company.com\",\n",
    "            \"manager@company.com\",\n",
    "            \"support@company.com\"\n",
    "        ]\n",
    "        \n",
    "        emails = []\n",
    "        for i in range(count):\n",
    "            emails.append({\n",
    "                'id': f'test_email_{i}',\n",
    "                'subject': random.choice(subjects).format(i),\n",
    "                'from': random.choice(from_addresses),\n",
    "                'body': f'Test email body {i} with some content...',\n",
    "                'received_at': (datetime.utcnow() - timedelta(hours=i)).isoformat()\n",
    "            })\n",
    "        return emails\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_csv_dataset() -> str:\n",
    "        \"\"\"Generate test CSV data.\"\"\"\n",
    "        csv_lines = ['date,product,sales,region']\n",
    "        for i in range(100):\n",
    "            date = (datetime.utcnow() - timedelta(days=100-i)).strftime('%Y-%m-%d')\n",
    "            product = random.choice(['Product A', 'Product B', 'Product C'])\n",
    "            sales = random.randint(500, 5000)\n",
    "            region = random.choice(['North', 'South', 'East', 'West'])\n",
    "            csv_lines.append(f'{date},{product},{sales},{region}')\n",
    "        return '\\n'.join(csv_lines)\n",
    "\n",
    "# Acceptance test runner\n",
    "class AcceptanceTestRunner:\n",
    "    \"\"\"Run all acceptance tests in sequence.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, verbose: bool = True):\n",
    "        self.base_url = base_url\n",
    "        self.verbose = verbose\n",
    "        self.results = []\n",
    "    \n",
    "    def run_all(self) -> bool:\n",
    "        \"\"\"Run all tests. Return True if all pass.\"\"\"\n",
    "        tests = [\n",
    "            ('Login & OAuth Linking', test_user_login_and_email_linking),\n",
    "            ('Email Batch Processing', test_email_batch_processing),\n",
    "            ('Auto-Reply Workflow', test_auto_reply_workflow),\n",
    "            ('Task Scheduling', test_follow_up_scheduling),\n",
    "            ('Data Analysis Job', test_data_analysis_job),\n",
    "            ('Audit Logging', test_audit_logs),\n",
    "        ]\n",
    "        \n",
    "        for test_name, test_func in tests:\n",
    "            try:\n",
    "                test_func()\n",
    "                self.results.append((test_name, 'PASS'))\n",
    "                if self.verbose:\n",
    "                    print(f\"✓ {test_name}\")\n",
    "            except AssertionError as e:\n",
    "                self.results.append((test_name, f'FAIL: {str(e)}'))\n",
    "                if self.verbose:\n",
    "                    print(f\"✗ {test_name}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                self.results.append((test_name, f'ERROR: {str(e)}'))\n",
    "                if self.verbose:\n",
    "                    print(f\"✗ {test_name} (ERROR): {str(e)}\")\n",
    "        \n",
    "        return all(result[1] == 'PASS' for result in self.results)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print test summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ACCEPTANCE TEST SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        for test_name, result in self.results:\n",
    "            status = \"✓ PASS\" if result == 'PASS' else \"✗ \" + result\n",
    "            print(f\"{test_name:<40} {status}\")\n",
    "        \n",
    "        passed = sum(1 for _, r in self.results if r == 'PASS')\n",
    "        total = len(self.results)\n",
    "        print(\"-\"*50)\n",
    "        print(f\"TOTAL: {passed}/{total} tests passed\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Run tests:\n",
    "# if __name__ == '__main__':\n",
    "#     runner = AcceptanceTestRunner('http://localhost:8000')\n",
    "#     all_pass = runner.run_all()\n",
    "#     runner.print_summary()\n",
    "#     exit(0 if all_pass else 1)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f155820",
   "metadata": {},
   "source": [
    "# Section 9: Master TODO List & Scaffolding\n",
    "\n",
    "## Complete Master TODO List (Granular, Ordered)\n",
    "\n",
    "### PHASE A: MVP Scoping & Repo Init\n",
    "\n",
    "#### A.1 Project Setup & Environment\n",
    "- [ ] **A.1.1** Create git repo on GitHub, initialize with README and .gitignore\n",
    "- [ ] **A.1.2** Create virtual environment: `python -m venv venv` + activate\n",
    "- [ ] **A.1.3** Create `requirements.txt` with: FastAPI, SQLAlchemy, Celery, Redis, LangChain, OpenAI, cryptography, psycopg2, python-dotenv\n",
    "- [ ] **A.1.4** Create `.env.example` with all required env vars (see section below)\n",
    "- [ ] **A.1.5** Create `.env` (local) from `.env.example` with test values\n",
    "- [ ] **A.1.6** Create `docker-compose.yml` with services: backend, worker, postgres, redis, frontend stub\n",
    "- [ ] **A.1.7** Create `Dockerfile` for Python backend with multi-stage build\n",
    "\n",
    "#### A.2 Backend Core Setup\n",
    "- [ ] **A.2.1** Create `backend/main.py` FastAPI app with health endpoint\n",
    "- [ ] **A.2.2** Create `backend/config.py` for environment variable loading\n",
    "- [ ] **A.2.3** Create `backend/models.py` with Postgres schema: User, UserEmailAccount, EmailJob, Task, AuditLog, Template, Rule\n",
    "- [ ] **A.2.4** Create `backend/database.py` with SQLAlchemy session factory\n",
    "- [ ] **A.2.5** Create `backend/api/health.py` with `/health` endpoint\n",
    "- [ ] **A.2.6** Create `backend/security/encryption.py` for token encryption/decryption\n",
    "\n",
    "#### A.3 Authentication\n",
    "- [ ] **A.3.1** Create `backend/api/auth.py` with `/register`, `/login`, `/logout` endpoints\n",
    "- [ ] **A.3.2** Implement JWT token generation and validation\n",
    "- [ ] **A.3.3** Create `backend/security/jwt.py` with encode/decode functions\n",
    "- [ ] **A.3.4** Create user model with password hashing (bcrypt)\n",
    "- [ ] **A.3.5** Add authentication middleware to check JWT tokens\n",
    "\n",
    "#### A.4 Email Integration (OAuth Flow)\n",
    "- [ ] **A.4.1** Create `backend/connectors/base.py` with `BaseEmailConnector` interface\n",
    "- [ ] **A.4.2** Create `backend/connectors/email_adapters.py` with `GmailConnector`, `OutlookConnector`, `IMAPConnector` stubs\n",
    "- [ ] **A.4.3** Create `backend/connectors/factory.py` with connector registration and creation\n",
    "- [ ] **A.4.4** Implement Gmail OAuth2 flow: `/api/email/oauth-authorize`, `/api/email/oauth-callback`\n",
    "- [ ] **A.4.5** Test email account linking with Gmail sandbox account\n",
    "- [ ] **A.4.6** Store encrypted OAuth tokens in DB (UserEmailAccount table)\n",
    "\n",
    "#### A.5 Email Processing Pipeline (Basic)\n",
    "- [ ] **A.5.1** Create `backend/worker/celery_app.py` with Celery config\n",
    "- [ ] **A.5.2** Create `backend/worker/tasks/email_processor.py` with `process_email` task stub\n",
    "- [ ] **A.5.3** Implement email fetch via Gmail API\n",
    "- [ ] **A.5.4** Create `backend/llm/embeddings.py` with OpenAI embedding function\n",
    "- [ ] **A.5.5** Create `backend/storage/vector_store.py` with FAISS index initialization\n",
    "- [ ] **A.5.6** Create basic classification chain (LangChain) in `backend/llm/classifier_chain.py`\n",
    "- [ ] **A.5.7** Implement email classification: label + confidence score\n",
    "- [ ] **A.5.8** Add endpoint to trigger email processing: `/api/emails/process`\n",
    "\n",
    "#### A.6 Data Analysis (MVP)\n",
    "- [ ] **A.6.1** Create `backend/connectors/data_adapters.py` with `CSVConnector`, `BaseDataConnector`\n",
    "- [ ] **A.6.2** Create `backend/worker/tasks/data_analysis.py` with basic stats job\n",
    "- [ ] **A.6.3** Implement CSV upload endpoint: `/api/data/upload`\n",
    "- [ ] **A.6.4** Create analysis worker: read CSV → compute stats → generate LLM summary\n",
    "- [ ] **A.6.5** Store results in DB and return summary + report link\n",
    "\n",
    "#### A.7 Frontend Scaffold (Next.js)\n",
    "- [ ] **A.7.1** Initialize Next.js app with TailwindCSS\n",
    "- [ ] **A.7.2** Create `pages/index.tsx` (dashboard home)\n",
    "- [ ] **A.7.3** Create `pages/inbox.tsx` (email list + classify button)\n",
    "- [ ] **A.7.4** Create `pages/login.tsx` (authentication page)\n",
    "- [ ] **A.7.5** Create `pages/analysis.tsx` (CSV upload + results)\n",
    "- [ ] **A.7.6** Create `components/Header.tsx`, `components/Sidebar.tsx`\n",
    "- [ ] **A.7.7** Add API client utility (`lib/api.ts`) for HTTP requests to backend\n",
    "\n",
    "#### A.8 Documentation & Testing\n",
    "- [ ] **A.8.1** Write README.md: overview, setup instructions, running locally\n",
    "- [ ] **A.8.2** Create `SETUP.md`: environment variables, database migrations, secrets config\n",
    "- [ ] **A.8.3** Create `tests/test_auth.py` with basic auth tests\n",
    "- [ ] **A.8.4** Create `tests/test_email_processor.py` with email fetch test\n",
    "- [ ] **A.8.5** Test entire MVP flow locally: register → login → link email → process emails → see results\n",
    "\n",
    "---\n",
    "\n",
    "### PHASE B: Core Features & Dashboard\n",
    "\n",
    "#### B.1 Celery Scheduler & Task Management\n",
    "- [ ] **B.1.1** Implement task scheduling with Celery Beat (cron jobs)\n",
    "- [ ] **B.1.2** Create email polling task: run every 5 minutes\n",
    "- [ ] **B.1.3** Implement task retry logic (exponential backoff, max 3 retries)\n",
    "- [ ] **B.1.4** Add task status tracking: pending → running → completed/failed\n",
    "- [ ] **B.1.5** Create `/api/jobs` endpoint to view job status\n",
    "\n",
    "#### B.2 Pre-Approved Reply System\n",
    "- [ ] **B.2.1** Create Template model with versioning\n",
    "- [ ] **B.2.2** Create `/api/templates` CRUD endpoints\n",
    "- [ ] **B.2.3** Implement template approval flow (requires user confirmation)\n",
    "- [ ] **B.2.4** Create `backend/worker/tasks/auto_reply.py` with safety gates:\n",
    "  - [ ] **B.2.4.a** Confidence threshold check (> 0.85)\n",
    "  - [ ] **B.2.4.b** Daily send limit enforcement\n",
    "  - [ ] **B.2.4.c** Sensitive topic detection\n",
    "  - [ ] **B.2.4.d** Template approval verification\n",
    "- [ ] **B.2.5** Implement LLM-based template substitution (personalization)\n",
    "- [ ] **B.2.6** Create `/api/templates/{id}/approve` endpoint\n",
    "- [ ] **B.2.7** Test auto-reply with test email account\n",
    "\n",
    "#### B.3 Rules Engine (YAML/JSON DSL)\n",
    "- [ ] **B.3.1** Create Rule model with YAML/JSON storage\n",
    "- [ ] **B.3.2** Create `backend/engine/rules_engine.py` evaluator\n",
    "- [ ] **B.3.3** Implement condition evaluation: contains, regex, similarity, domain, etc.\n",
    "- [ ] **B.3.4** Support action types: apply_label, flag, auto_reply, create_task\n",
    "- [ ] **B.3.5** Create `/api/rules` CRUD endpoints\n",
    "- [ ] **B.3.6** Integrate rules into email processor pipeline\n",
    "\n",
    "#### B.4 Vector Store & Retrieval (FAISS)\n",
    "- [ ] **B.4.1** Implement FAISS index builder in `backend/storage/vector_store.py`\n",
    "- [ ] **B.4.2** Create background task to build/update index from emails\n",
    "- [ ] **B.4.3** Implement retrieval chain: query vector store → find similar emails + rules\n",
    "- [ ] **B.4.4** Integrate retriever into classifier chain (context augmentation)\n",
    "\n",
    "#### B.5 Dashboard Pages & UI\n",
    "- [ ] **B.5.1** Build inbox preview page: show emails, labels, confidence scores\n",
    "- [ ] **B.5.2** Build template manager page: create, edit, approve templates\n",
    "- [ ] **B.5.3** Build rule builder page: visual editor for YAML rules\n",
    "- [ ] **B.5.4** Build job launcher page: select analysis type, upload data, view results\n",
    "- [ ] **B.5.5** Build logs page: view job history, errors, audit trail\n",
    "- [ ] **B.5.6** Add real-time status updates (WebSocket or polling)\n",
    "\n",
    "#### B.6 Integration Testing\n",
    "- [ ] **B.6.1** Create `tests/test_rules_engine.py` with rule evaluation tests\n",
    "- [ ] **B.6.2** Create `tests/test_auto_reply.py` with safety gate tests\n",
    "- [ ] **B.6.3** Create `tests/test_data_analysis.py` with analysis job tests\n",
    "- [ ] **B.6.4** Integration test: email → rule match → auto-reply sent\n",
    "\n",
    "---\n",
    "\n",
    "### PHASE C: Hardening & Deployment\n",
    "\n",
    "#### C.1 Security & Encryption\n",
    "- [ ] **C.1.1** Implement token encryption at rest (AES-256 + KMS-style key management)\n",
    "- [ ] **C.1.2** Create secrets management: load from .env or AWS Secrets Manager\n",
    "- [ ] **C.1.3** Add HTTPS support (Let's Encrypt + certbot)\n",
    "- [ ] **C.1.4** Implement CORS policy\n",
    "- [ ] **C.1.5** Add rate limiting to API endpoints\n",
    "- [ ] **C.1.6** Implement CSRF protection for POST endpoints\n",
    "\n",
    "#### C.2 Audit Logging\n",
    "- [ ] **C.2.1** Create audit logging middleware\n",
    "- [ ] **C.2.2** Log all critical actions: template approval, auto-reply sent, job created, etc.\n",
    "- [ ] **C.2.3** Encrypt sensitive details in audit logs\n",
    "- [ ] **C.2.4** Create `/api/audit-logs` endpoint (admin only)\n",
    "- [ ] **C.2.5** Implement data retention policy and cleanup task\n",
    "\n",
    "#### C.3 Observability & Monitoring\n",
    "- [ ] **C.3.1** Integrate Sentry error tracking\n",
    "- [ ] **C.3.2** Add Prometheus metrics exports (job duration, email count, LLM costs)\n",
    "- [ ] **C.3.3** Create basic Grafana dashboard\n",
    "- [ ] **C.3.4** Add structured logging (JSON format for easy parsing)\n",
    "- [ ] **C.3.5** Implement health check endpoints for monitoring\n",
    "\n",
    "#### C.4 Testing & Quality\n",
    "- [ ] **C.4.1** Expand unit tests (aim for 80%+ coverage)\n",
    "- [ ] **C.4.2** Create integration tests for all API endpoints\n",
    "- [ ] **C.4.3** Create `tests/test_acceptance.py` with full end-to-end acceptance tests\n",
    "- [ ] **C.4.4** Add load testing script (simulate 100 emails/min)\n",
    "- [ ] **C.4.5** Set up code quality checks: linting (flake8), formatting (black), type checking (mypy)\n",
    "\n",
    "#### C.5 Docker & Deployment\n",
    "- [ ] **C.5.1** Finalize Dockerfile with security best practices\n",
    "- [ ] **C.5.2** Finalize docker-compose.yml with all services\n",
    "- [ ] **C.5.3** Add database migration script (Alembic)\n",
    "- [ ] **C.5.4** Test full deployment locally: `docker-compose up` → all healthy\n",
    "- [ ] **C.5.5** Create deployment guide with environment variables checklist\n",
    "\n",
    "#### C.6 CI/CD Pipeline\n",
    "- [ ] **C.6.1** Create GitHub Actions workflow for: lint, test, build Docker image\n",
    "- [ ] **C.6.2** Push Docker image to Docker Hub or GitHub Container Registry\n",
    "- [ ] **C.6.3** Automate deployment (optional: push to DigitalOcean App Platform)\n",
    "- [ ] **C.6.4** Set up staging environment for testing\n",
    "\n",
    "#### C.7 Acceptance Testing\n",
    "- [ ] **C.7.1** Run full acceptance test harness (6 tests from Section 8)\n",
    "- [ ] **C.7.2** Verify all acceptance criteria pass\n",
    "- [ ] **C.7.3** Create demo script for hand-off\n",
    "- [ ] **C.7.4** Document acceptance test results\n",
    "\n",
    "---\n",
    "\n",
    "### PHASE D: Optional Scaling & Enterprise\n",
    "\n",
    "#### D.1 Managed Vector Database\n",
    "- [ ] **D.1.1** Evaluate Pinecone API\n",
    "- [ ] **D.1.2** Create migration script: export FAISS → import to Pinecone\n",
    "- [ ] **D.1.3** Update retriever code to use Pinecone client\n",
    "- [ ] **D.1.4** Test retrieval performance\n",
    "\n",
    "#### D.2 Advanced Task Scheduling (Temporal)\n",
    "- [ ] **D.2.1** Evaluate Temporal/Temporal Cloud\n",
    "- [ ] **D.2.2** Create Temporal workflow definitions for: email processing, data analysis\n",
    "- [ ] **D.2.3** Migrate from Celery to Temporal workers\n",
    "- [ ] **D.2.4** Test durable execution and retry guarantees\n",
    "\n",
    "#### D.3 Kubernetes Deployment\n",
    "- [ ] **D.3.1** Create Helm charts for backend, worker, postgres, redis\n",
    "- [ ] **D.3.2** Set up k8s namespaces and resource limits\n",
    "- [ ] **D.3.3** Implement auto-scaling: HPA based on job queue depth\n",
    "- [ ] **D.3.4** Deploy to GKE/EKS/DigitalOcean k8s\n",
    "- [ ] **D.3.5** Set up Prometheus + Grafana on k8s\n",
    "\n",
    "#### D.4 Role-Based Access Control (RBAC)\n",
    "- [ ] **D.4.1** Create Role and Permission models\n",
    "- [ ] **D.4.2** Implement RBAC middleware for API endpoints\n",
    "- [ ] **D.4.3** Create roles: admin, approver, user\n",
    "- [ ] **D.4.4** Add role management UI (admin only)\n",
    "\n",
    "#### D.5 Advanced Data Connectors\n",
    "- [ ] **D.5.1** Create Google Sheets connector\n",
    "- [ ] **D.5.2** Create S3 connector\n",
    "- [ ] **D.5.3** Create SQL database connector (Postgres/MySQL)\n",
    "- [ ] **D.5.4** Create BigQuery connector (optional)\n",
    "\n",
    "#### D.6 Enterprise Features\n",
    "- [ ] **D.6.1** Implement GDPR compliance: data export, deletion\n",
    "- [ ] **D.6.2** Add multi-tenancy support (separate data per customer)\n",
    "- [ ] **D.6.3** Create billing/metering system\n",
    "- [ ] **D.6.4** Implement SSO (SAML/OIDC) support\n",
    "\n",
    "---\n",
    "\n",
    "## Scaffolding Commands (Quick Start)\n",
    "\n",
    "### Phase A Quick Setup\n",
    "\n",
    "```bash\n",
    "# 1. Create project directory and virtual environment\n",
    "cd /path/to/projects\n",
    "mkdir virtual-assistant-scheduler\n",
    "cd virtual-assistant-scheduler\n",
    "\n",
    "# 2. Create virtual environment\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "\n",
    "# 3. Clone repo or initialize git\n",
    "git init\n",
    "\n",
    "# 4. Create directory structure\n",
    "mkdir -p backend/{api,connectors,llm,security,storage,worker/tasks}\n",
    "mkdir -p frontend\n",
    "mkdir -p tests\n",
    "mkdir -p docker\n",
    "\n",
    "# 5. Create initial files\n",
    "touch backend/__init__.py\n",
    "touch backend/main.py\n",
    "touch backend/config.py\n",
    "touch backend/models.py\n",
    "touch backend/database.py\n",
    "touch requirements.txt\n",
    "touch .env.example\n",
    "touch docker-compose.yml\n",
    "touch Dockerfile\n",
    "touch README.md\n",
    "\n",
    "# 6. Install dependencies\n",
    "pip install fastapi uvicorn sqlalchemy psycopg2-binary celery redis langchain openai cryptography python-dotenv pydantic\n",
    "\n",
    "# 7. Create Dockerfile (see section below)\n",
    "# ... (see Dockerfile content below)\n",
    "\n",
    "# 8. Create docker-compose.yml (see section below)\n",
    "# ... (see docker-compose.yml content below)\n",
    "\n",
    "# 9. Run locally\n",
    "docker-compose up --build\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04d31a",
   "metadata": {},
   "source": [
    "## Key Scaffolding Files\n",
    "\n",
    "### 1. `.env.example`\n",
    "\n",
    "```env\n",
    "# Backend Config\n",
    "FASTAPI_ENV=development\n",
    "FASTAPI_DEBUG=true\n",
    "SECRET_KEY=your-secret-key-here-change-in-production\n",
    "\n",
    "# Database\n",
    "DATABASE_URL=postgresql://user:password@postgres:5432/ai_agent_db\n",
    "SQLALCHEMY_ECHO=true\n",
    "\n",
    "# Redis & Celery\n",
    "REDIS_URL=redis://redis:6379/0\n",
    "CELERY_BROKER_URL=redis://redis:6379/0\n",
    "CELERY_RESULT_BACKEND=redis://redis:6379/0\n",
    "\n",
    "# OpenAI\n",
    "OPENAI_API_KEY=sk-...\n",
    "OPENAI_MODEL=gpt-3.5-turbo\n",
    "OPENAI_EMBEDDING_MODEL=text-embedding-3-small\n",
    "\n",
    "# Gmail OAuth (get from Google Cloud Console)\n",
    "GMAIL_CLIENT_ID=xxx.apps.googleusercontent.com\n",
    "GMAIL_CLIENT_SECRET=xxx\n",
    "GMAIL_REDIRECT_URI=http://localhost:8000/api/email/oauth-callback/gmail\n",
    "\n",
    "# Outlook OAuth (get from Azure Portal)\n",
    "OUTLOOK_CLIENT_ID=xxx\n",
    "OUTLOOK_CLIENT_SECRET=xxx\n",
    "\n",
    "# Encryption\n",
    "ENCRYPTION_KEY=your-base64-encoded-fernet-key  # Generate with: from cryptography.fernet import Fernet; print(Fernet.generate_key())\n",
    "\n",
    "# AWS/S3 (if using)\n",
    "AWS_ACCESS_KEY_ID=xxx\n",
    "AWS_SECRET_ACCESS_KEY=xxx\n",
    "AWS_S3_BUCKET=ai-agent-reports\n",
    "AWS_REGION=us-east-1\n",
    "\n",
    "# Sentry (optional)\n",
    "SENTRY_DSN=https://...\n",
    "\n",
    "# Frontend\n",
    "NEXT_PUBLIC_API_BASE_URL=http://localhost:8000/api\n",
    "```\n",
    "\n",
    "### 2. `Dockerfile`\n",
    "\n",
    "```dockerfile\n",
    "# Multi-stage build for backend\n",
    "\n",
    "FROM python:3.11-slim as builder\n",
    "\n",
    "WORKDIR /tmp\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    gcc \\\n",
    "    postgresql-client \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --user --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Final stage\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    postgresql-client \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"uvicorn\", \"backend.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "### 3. `docker-compose.yml`\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  postgres:\n",
    "    image: postgres:15-alpine\n",
    "    environment:\n",
    "      POSTGRES_USER: ai_agent_user\n",
    "      POSTGRES_PASSWORD: ai_agent_password\n",
    "      POSTGRES_DB: ai_agent_db\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"pg_isready -U ai_agent_user\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"ping\"]\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "\n",
    "  backend:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    command: uvicorn backend.main:app --host 0.0.0.0 --port 8000 --reload\n",
    "    env_file: .env\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    depends_on:\n",
    "      postgres:\n",
    "        condition: service_healthy\n",
    "      redis:\n",
    "        condition: service_healthy\n",
    "    volumes:\n",
    "      - .:/app\n",
    "    environment:\n",
    "      - DATABASE_URL=postgresql://ai_agent_user:ai_agent_password@postgres:5432/ai_agent_db\n",
    "      - REDIS_URL=redis://redis:6379/0\n",
    "\n",
    "  worker:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    command: celery -A backend.worker.celery_app worker -l info\n",
    "    env_file: .env\n",
    "    depends_on:\n",
    "      - redis\n",
    "      - postgres\n",
    "    volumes:\n",
    "      - .:/app\n",
    "    environment:\n",
    "      - DATABASE_URL=postgresql://ai_agent_user:ai_agent_password@postgres:5432/ai_agent_db\n",
    "      - REDIS_URL=redis://redis:6379/0\n",
    "\n",
    "  frontend:\n",
    "    build:\n",
    "      context: ./frontend\n",
    "      dockerfile: Dockerfile\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    depends_on:\n",
    "      - backend\n",
    "    environment:\n",
    "      - NEXT_PUBLIC_API_BASE_URL=http://localhost:8000/api\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  redis_data:\n",
    "```\n",
    "\n",
    "### 4. `backend/main.py`\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from contextlib import asynccontextmanager\n",
    "import logging\n",
    "\n",
    "from backend.config import settings\n",
    "from backend.database import engine, Base\n",
    "from backend.api import auth, email, jobs\n",
    "\n",
    "# Create tables\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Startup\n",
    "    logging.info(\"Starting up AI Agent Backend\")\n",
    "    yield\n",
    "    # Shutdown\n",
    "    logging.info(\"Shutting down\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"AI Agent API\",\n",
    "    description=\"Virtual Assistant & Task Scheduler\",\n",
    "    version=\"0.1.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "# CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\", \"http://localhost:8000\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Include routers\n",
    "app.include_router(auth.router, prefix=\"/api/auth\", tags=[\"auth\"])\n",
    "app.include_router(email.router, prefix=\"/api/email\", tags=[\"email\"])\n",
    "app.include_router(jobs.router, prefix=\"/api/jobs\", tags=[\"jobs\"])\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"service\": \"ai-agent-backend\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```\n",
    "\n",
    "### 5. `backend/worker/celery_app.py`\n",
    "\n",
    "```python\n",
    "from celery import Celery\n",
    "from backend.config import settings\n",
    "\n",
    "celery_app = Celery(\n",
    "    'ai_agent_worker',\n",
    "    broker=settings.REDIS_URL,\n",
    "    backend=settings.REDIS_URL\n",
    ")\n",
    "\n",
    "celery_app.conf.update(\n",
    "    task_serializer='json',\n",
    "    accept_content=['json'],\n",
    "    result_serializer='json',\n",
    "    timezone='UTC',\n",
    "    enable_utc=True,\n",
    "    task_track_started=True,\n",
    "    task_time_limit=30 * 60,  # 30 minutes\n",
    "    task_soft_time_limit=25 * 60,  # 25 minutes\n",
    ")\n",
    "\n",
    "# Auto-discover tasks from backend.worker.tasks\n",
    "celery_app.autodiscover_tasks(['backend.worker.tasks'])\n",
    "```\n",
    "\n",
    "### 6. `backend/models.py` (Key Schemas)\n",
    "\n",
    "```python\n",
    "from sqlalchemy import Column, Integer, String, Text, DateTime, JSON, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import relationship\n",
    "from backend.database import Base\n",
    "from datetime import datetime\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    email = Column(String(255), unique=True)\n",
    "    hashed_password = Column(String(255))\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    email_accounts = relationship(\"UserEmailAccount\", back_populates=\"user\")\n",
    "    email_jobs = relationship(\"EmailJob\", back_populates=\"user\")\n",
    "\n",
    "class UserEmailAccount(Base):\n",
    "    __tablename__ = \"user_email_accounts\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, ForeignKey(\"users.id\"))\n",
    "    provider = Column(String(50))  # 'gmail', 'outlook', 'imap'\n",
    "    email_address = Column(String(255))\n",
    "    encrypted_token = Column(Text)\n",
    "    scopes = Column(String(500))\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    user = relationship(\"User\", back_populates=\"email_accounts\")\n",
    "\n",
    "class EmailJob(Base):\n",
    "    __tablename__ = \"email_jobs\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, ForeignKey(\"users.id\"))\n",
    "    message_id = Column(String(500))\n",
    "    email_subject = Column(String(500))\n",
    "    email_from = Column(String(255))\n",
    "    email_body = Column(Text)\n",
    "    status = Column(String(50))  # 'pending', 'processed', 'failed'\n",
    "    classifier_confidence = Column(Float, default=0.0)\n",
    "    applied_labels = Column(String(500))  # Comma-separated\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    user = relationship(\"User\", back_populates=\"email_jobs\")\n",
    "\n",
    "class Task(Base):\n",
    "    __tablename__ = \"tasks\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, ForeignKey(\"users.id\"))\n",
    "    task_type = Column(String(50))  # 'email_process', 'auto_reply', 'data_analysis'\n",
    "    status = Column(String(50))  # 'pending', 'running', 'completed', 'failed'\n",
    "    celery_task_id = Column(String(255))\n",
    "    params = Column(JSON)\n",
    "    result = Column(JSON)\n",
    "    error_message = Column(Text)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "class AuditLog(Base):\n",
    "    __tablename__ = \"audit_logs\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, ForeignKey(\"users.id\"))\n",
    "    action = Column(String(100))  # 'email_processed', 'auto_reply_sent', 'template_approved'\n",
    "    resource_id = Column(Integer)\n",
    "    resource_type = Column(String(50))  # 'email', 'template', 'job'\n",
    "    _details_encrypted = Column(Text)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "class Template(Base):\n",
    "    __tablename__ = \"templates\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, ForeignKey(\"users.id\"))\n",
    "    name = Column(String(255))\n",
    "    subject = Column(String(500))\n",
    "    body = Column(Text)\n",
    "    approved_for_auto_send = Column(Boolean, default=False)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "class Rule(Base):\n",
    "    __tablename__ = \"rules\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, ForeignKey(\"users.id\"))\n",
    "    name = Column(String(255))\n",
    "    config = Column(Text)  # YAML/JSON rule definition\n",
    "    enabled = Column(Boolean, default=True)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "```\n",
    "\n",
    "### 7. `frontend/pages/index.tsx` (Next.js Home)\n",
    "\n",
    "```typescript\n",
    "import { useEffect, useState } from 'react';\n",
    "import Link from 'next/link';\n",
    "\n",
    "export default function Home() {\n",
    "  const [user, setUser] = useState(null);\n",
    "\n",
    "  useEffect(() => {\n",
    "    // Check if logged in\n",
    "    const token = localStorage.getItem('auth_token');\n",
    "    if (token) {\n",
    "      // Fetch user profile\n",
    "      fetch(`${process.env.NEXT_PUBLIC_API_BASE_URL}/auth/me`, {\n",
    "        headers: { 'Authorization': `Bearer ${token}` }\n",
    "      })\n",
    "        .then(r => r.json())\n",
    "        .then(data => setUser(data))\n",
    "        .catch(() => setUser(null));\n",
    "    }\n",
    "  }, []);\n",
    "\n",
    "  return (\n",
    "    <div className=\"min-h-screen bg-gradient-to-br from-blue-500 to-purple-600 text-white\">\n",
    "      <header className=\"p-4 border-b border-white/20\">\n",
    "        <h1 className=\"text-2xl font-bold\">AI Agent Dashboard</h1>\n",
    "      </header>\n",
    "\n",
    "      <main className=\"p-8\">\n",
    "        {!user ? (\n",
    "          <div className=\"text-center\">\n",
    "            <h2 className=\"text-4xl font-bold mb-4\">Welcome</h2>\n",
    "            <p className=\"mb-8 text-lg\">Virtual Assistant & Task Scheduler</p>\n",
    "            <Link href=\"/login\" className=\"bg-white text-blue-600 px-6 py-2 rounded-lg font-bold\">\n",
    "              Login\n",
    "            </Link>\n",
    "          </div>\n",
    "        ) : (\n",
    "          <div className=\"grid grid-cols-1 md:grid-cols-3 gap-6\">\n",
    "            <Link href=\"/inbox\" className=\"bg-white/20 p-6 rounded-lg hover:bg-white/30 transition\">\n",
    "              <h3 className=\"text-xl font-bold\">📧 Inbox</h3>\n",
    "              <p className=\"text-sm mt-2\">Manage and process emails</p>\n",
    "            </Link>\n",
    "            <Link href=\"/analysis\" className=\"bg-white/20 p-6 rounded-lg hover:bg-white/30 transition\">\n",
    "              <h3 className=\"text-xl font-bold\">📊 Data Analysis</h3>\n",
    "              <p className=\"text-sm mt-2\">Upload and analyze datasets</p>\n",
    "            </Link>\n",
    "            <Link href=\"/templates\" className=\"bg-white/20 p-6 rounded-lg hover:bg-white/30 transition\">\n",
    "              <h3 className=\"text-xl font-bold\">📝 Templates</h3>\n",
    "              <p className=\"text-sm mt-2\">Manage reply templates</p>\n",
    "            </Link>\n",
    "          </div>\n",
    "        )}\n",
    "      </main>\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938893b0",
   "metadata": {},
   "source": [
    "# Section 10: Checklists (PR, Release, Hand-off)\n",
    "\n",
    "## PR Checklist (Before Merging to Main)\n",
    "\n",
    "### Code Quality\n",
    "- [ ] Code follows PEP 8 style guidelines (run `black` formatter)\n",
    "- [ ] No linting errors (run `flake8` or `pylint`)\n",
    "- [ ] Type hints added to all functions (`mypy` clean)\n",
    "- [ ] No hardcoded secrets or credentials\n",
    "- [ ] No print() statements; use logging instead\n",
    "- [ ] Remove debug code and commented-out lines\n",
    "\n",
    "### Testing\n",
    "- [ ] Unit tests added for new functions (min 80% coverage)\n",
    "- [ ] Integration tests added for new API endpoints\n",
    "- [ ] All tests pass locally: `pytest tests/`\n",
    "- [ ] Load test passed (simulate expected traffic)\n",
    "- [ ] No flaky tests (tests pass consistently)\n",
    "\n",
    "### Security\n",
    "- [ ] No SQL injection vulnerabilities (use parameterized queries)\n",
    "- [ ] No XSS vulnerabilities in frontend\n",
    "- [ ] No exposed credentials in logs or error messages\n",
    "- [ ] CORS policy reviewed and secure\n",
    "- [ ] Input validation on all API endpoints\n",
    "- [ ] Rate limiting applied to sensitive endpoints\n",
    "\n",
    "### Documentation\n",
    "- [ ] Docstrings added to all public functions\n",
    "- [ ] README updated if new setup steps required\n",
    "- [ ] API endpoints documented (in code or OpenAPI)\n",
    "- [ ] Complex logic has inline comments\n",
    "- [ ] Changelog updated (if applicable)\n",
    "\n",
    "### Performance\n",
    "- [ ] Database queries optimized (no N+1 queries)\n",
    "- [ ] No unnecessary loops or recursion\n",
    "- [ ] Cache headers set on static assets\n",
    "- [ ] API response time < 500ms for most endpoints\n",
    "\n",
    "### Database\n",
    "- [ ] Schema migrations created (Alembic)\n",
    "- [ ] Backward compatibility maintained\n",
    "- [ ] Indexes added on frequently queried columns\n",
    "- [ ] No data loss on rollback\n",
    "\n",
    "### Deployment Readiness\n",
    "- [ ] Docker image builds without errors\n",
    "- [ ] docker-compose.yml updated if services changed\n",
    "- [ ] .env.example updated with new variables\n",
    "- [ ] Health check endpoints working\n",
    "- [ ] Graceful shutdown handled\n",
    "\n",
    "### Review Checklist\n",
    "- [ ] At least 1 code review approval\n",
    "- [ ] All review comments addressed\n",
    "- [ ] CI/CD pipeline passes (GitHub Actions)\n",
    "- [ ] No merge conflicts with main branch\n",
    "- [ ] Commit history clean (meaningful messages)\n",
    "\n",
    "---\n",
    "\n",
    "## Release Checklist (MVP Ready)\n",
    "\n",
    "### Pre-Release Testing\n",
    "- [ ] Full acceptance test harness passes (6/6 tests)\n",
    "- [ ] Manual smoke test on staging environment\n",
    "- [ ] Database backups taken\n",
    "- [ ] Rollback plan documented\n",
    "- [ ] Load testing completed (100+ concurrent users)\n",
    "- [ ] Security audit passed\n",
    "- [ ] Vulnerability scan passed (no critical issues)\n",
    "\n",
    "### Documentation & Knowledge\n",
    "- [ ] README.md complete and up-to-date\n",
    "- [ ] SETUP.md with step-by-step deployment instructions\n",
    "- [ ] DEPLOYMENT.md with production checklist\n",
    "- [ ] API documentation complete (Swagger/OpenAPI)\n",
    "- [ ] Admin runbook created (how to manage email providers, templates, workers)\n",
    "- [ ] Troubleshooting guide written (common issues + solutions)\n",
    "- [ ] Demo script prepared and tested\n",
    "\n",
    "### Infrastructure & Monitoring\n",
    "- [ ] Production environment configured\n",
    "- [ ] Environment variables securely managed (secrets manager)\n",
    "- [ ] Database backups automated (daily)\n",
    "- [ ] Log aggregation set up (Sentry / ELK)\n",
    "- [ ] Monitoring dashboards created (Prometheus/Grafana)\n",
    "- [ ] Alerts configured for critical failures\n",
    "- [ ] Status page set up (for incidents)\n",
    "\n",
    "### Compliance & Security\n",
    "- [ ] HTTPS/TLS enabled with valid certificates\n",
    "- [ ] Token encryption verified\n",
    "- [ ] Audit logging enabled and tested\n",
    "- [ ] GDPR data retention policy implemented\n",
    "- [ ] Data privacy policy written and agreed\n",
    "- [ ] Security vulnerability disclosure policy created\n",
    "- [ ] Rate limiting and DDoS protection configured\n",
    "\n",
    "### Release Artifacts\n",
    "- [ ] Docker images tagged and pushed to registry\n",
    "- [ ] Version number bumped (0.1.0)\n",
    "- [ ] Release notes written (features, fixes, known issues)\n",
    "- [ ] Changelog updated\n",
    "- [ ] Git tag created (v0.1.0)\n",
    "- [ ] Release announcement prepared (blog/email/Slack)\n",
    "\n",
    "### Go/No-Go Decision\n",
    "- [ ] Executive stakeholders signed off\n",
    "- [ ] Product requirements met (feature complete)\n",
    "- [ ] Performance benchmarks met\n",
    "- [ ] Security review passed\n",
    "- [ ] Legal/compliance review passed\n",
    "- [ ] Support team trained on new features\n",
    "- [ ] Decision: **GO** / **NO-GO**\n",
    "\n",
    "---\n",
    "\n",
    "## Hand-Off Checklist (What to Deliver & Demo)\n",
    "\n",
    "### Code & Repository\n",
    "- [ ] **Deliverable**: Git repository with clean commit history\n",
    "  - [ ] All code on `main` branch\n",
    "  - [ ] Tagged release version (e.g., `v0.1.0-mvp`)\n",
    "  - [ ] .gitignore properly configured\n",
    "  - [ ] README links to all documentation\n",
    "  \n",
    "- [ ] **Deliverable**: Docker image in registry (Docker Hub or GCR)\n",
    "  - [ ] Image tagged `ai-agent:0.1.0`\n",
    "  - [ ] Image is reproducible (deterministic builds)\n",
    "  - [ ] Image scanned for vulnerabilities\n",
    "\n",
    "### Documentation Package\n",
    "\n",
    "- [ ] **SETUP.md** (Step-by-step local setup)\n",
    "  ```markdown\n",
    "  1. Clone repo\n",
    "  2. Create virtual environment\n",
    "  3. Copy .env.example → .env and fill in values\n",
    "  4. docker-compose up\n",
    "  5. Open http://localhost:3000\n",
    "  ```\n",
    "\n",
    "- [ ] **DEPLOYMENT.md** (Production deployment)\n",
    "  ```markdown\n",
    "  1. Choose hosting provider (DigitalOcean/AWS/etc.)\n",
    "  2. Set up secrets manager\n",
    "  3. Run database migrations\n",
    "  4. Deploy via Docker Compose or k8s\n",
    "  5. Verify health checks\n",
    "  6. Update DNS records\n",
    "  ```\n",
    "\n",
    "- [ ] **ADMIN_RUNBOOK.md** (How to operate)\n",
    "  ```markdown\n",
    "  - How to link Gmail/Outlook accounts\n",
    "  - How to create and approve templates\n",
    "  - How to define rules\n",
    "  - How to monitor worker health\n",
    "  - How to view logs and audit trail\n",
    "  - How to scale worker pool\n",
    "  - Troubleshooting guide\n",
    "  ```\n",
    "\n",
    "- [ ] **API_DOCUMENTATION.md**\n",
    "  - All endpoints documented (GET, POST, PUT, DELETE)\n",
    "  - Request/response examples\n",
    "  - Error codes explained\n",
    "  - Rate limiting documented\n",
    "\n",
    "- [ ] **ARCHITECTURE.md** (System design overview)\n",
    "  - Diagram of system components\n",
    "  - Data flow explanation\n",
    "  - Extension points for new automations\n",
    "  - Technology stack rationale\n",
    "\n",
    "- [ ] **COST_ANALYSIS.md**\n",
    "  - Monthly cost breakdown\n",
    "  - Cost optimization strategies\n",
    "  - Pricing recommendations for customers\n",
    "\n",
    "- [ ] **SECURITY.md**\n",
    "  - OAuth setup instructions\n",
    "  - Token encryption details\n",
    "  - How audit logs work\n",
    "  - Data retention policies\n",
    "  - Compliance checklist\n",
    "\n",
    "### Demo & Acceptance Tests\n",
    "\n",
    "#### Demo Script (15 minutes)\n",
    "\n",
    "```\n",
    "1. Show dashboard login (2 min)\n",
    "   - Register new user\n",
    "   - Login with credentials\n",
    "   - Show home page\n",
    "\n",
    "2. Link email account (3 min)\n",
    "   - Click \"Link Email\"\n",
    "   - Show OAuth flow to Gmail\n",
    "   - Confirm token stored encrypted\n",
    "\n",
    "3. Process test emails (3 min)\n",
    "   - Show test emails in inbox\n",
    "   - Click \"Process Emails\"\n",
    "   - Show classification results + confidence scores\n",
    "   - Show applied labels\n",
    "\n",
    "4. Approve template & test auto-reply (4 min)\n",
    "   - Create email template\n",
    "   - Show approval workflow\n",
    "   - Create rule to trigger auto-reply\n",
    "   - Send test email to mailbox\n",
    "   - Show auto-reply sent + audit log entry\n",
    "\n",
    "5. Data analysis job (3 min)\n",
    "   - Upload test CSV\n",
    "   - Click \"Analyze\"\n",
    "   - Show LLM-generated summary\n",
    "   - Download report (PDF/HTML)\n",
    "```\n",
    "\n",
    "#### Acceptance Test Results\n",
    "\n",
    "Run and pass all 6 acceptance tests:\n",
    "\n",
    "1. ✓ **User login & OAuth email linking**\n",
    "2. ✓ **Email batch processing & classification**\n",
    "3. ✓ **Pre-approved template & auto-reply**\n",
    "4. ✓ **Follow-up task scheduling**\n",
    "5. ✓ **Data-analysis job end-to-end**\n",
    "6. ✓ **Audit log verification**\n",
    "\n",
    "**Execute**: `pytest tests/test_acceptance.py -v`\n",
    "\n",
    "### Hand-Off Call Agenda (45 minutes)\n",
    "\n",
    "**15 min: System Overview**\n",
    "- Architecture diagram walkthrough\n",
    "- Tech stack and why those choices\n",
    "- Cost breakdown\n",
    "\n",
    "**20 min: Live Demo**\n",
    "- Run demo script (see above)\n",
    "- Show logs and monitoring dashboard\n",
    "- Answer technical questions\n",
    "\n",
    "**10 min: Operations & Support**\n",
    "- How to scale workers\n",
    "- How to monitor system health\n",
    "- Where to get help (docs, GitHub issues, Slack)\n",
    "\n",
    "**Optional: Q&A**\n",
    "- Discuss future enhancements (Phase D)\n",
    "- Talk about migration paths\n",
    "\n",
    "### Deliverables Checklist\n",
    "\n",
    "**Before hand-off call:**\n",
    "\n",
    "- [ ] Code repository (GitHub/GitLab)\n",
    "- [ ] Docker image in registry\n",
    "- [ ] SETUP.md (local development)\n",
    "- [ ] DEPLOYMENT.md (production)\n",
    "- [ ] ADMIN_RUNBOOK.md (operations)\n",
    "- [ ] API_DOCUMENTATION.md\n",
    "- [ ] ARCHITECTURE.md\n",
    "- [ ] COST_ANALYSIS.md\n",
    "- [ ] SECURITY.md\n",
    "- [ ] Demo script (tested & working)\n",
    "- [ ] Acceptance test results (screenshots or video)\n",
    "- [ ] Presentation slides (15-20 slides)\n",
    "- [ ] Post-hand-off support agreement (SLA, response time)\n",
    "\n",
    "**During hand-off call:**\n",
    "\n",
    "- [ ] Present system overview (no more than 15 min)\n",
    "- [ ] Perform live demo (exactly as scripted)\n",
    "- [ ] Answer questions about architecture & operations\n",
    "- [ ] Show monitoring & alerting setup\n",
    "- [ ] Discuss scaling strategy\n",
    "\n",
    "**After hand-off call:**\n",
    "\n",
    "- [ ] Provide access to all systems (GitHub, Docker, AWS, monitoring)\n",
    "- [ ] Create admin user for recipient\n",
    "- [ ] Set up Slack or email channel for support\n",
    "- [ ] Schedule first follow-up call (1 week post-handoff)\n",
    "\n",
    "---\n",
    "\n",
    "## Post-MVP Roadmap (Optional)\n",
    "\n",
    "### Immediate Actions (Week 1-2 post-launch)\n",
    "- [ ] Monitor system health (errors, performance, uptime)\n",
    "- [ ] Gather user feedback on email linking, template approval, auto-reply safety\n",
    "- [ ] Fix any critical bugs found in production\n",
    "- [ ] Optimize slow API endpoints (if any)\n",
    "\n",
    "### Phase B Kickoff (Weeks 3-6)\n",
    "- [ ] Begin work on scheduler improvements\n",
    "- [ ] Start building advanced dashboard features\n",
    "- [ ] Add multi-email account support\n",
    "- [ ] Plan vector store integration (FAISS → Pinecone)\n",
    "\n",
    "### Phase C Kickoff (Weeks 7-12)\n",
    "- [ ] Hardening: security audit, encryption, audit logs\n",
    "- [ ] Scale testing: prepare for 10x traffic\n",
    "- [ ] Enterprise features: RBAC, billing, SSO\n",
    "- [ ] Plan Kubernetes deployment\n",
    "\n",
    "### Phase D Kickoff (Weeks 13+)\n",
    "- [ ] Scale to managed services\n",
    "- [ ] Add advanced connectors (Salesforce, BigQuery)\n",
    "- [ ] Multi-tenancy support\n",
    "- [ ] Prepare for enterprise sales\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
